model_type: transformer
d_model: 64
nhead: 4
num_layers: 2
dim_feedforward: 128
dropout: 0.2
epochs: 30
learning_rate: 0.0005
batch_size: 256
