{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Baseball Pitch Sequence Prediction","text":"<p>A professional-grade Python package for baseball pitch sequence prediction using 7 ML models, with benchmarking, ablation studies, and MLflow experiment tracking.</p>"},{"location":"#overview","title":"Overview","text":"<p>This project generates synthetic baseball pitch data with realistic pitcher archetypes, pitch sequence strategies, fatigue modeling, and game situation context \u2014 then trains and compares multiple models for predicting the next pitch type.</p>"},{"location":"#models","title":"Models","text":"Model Type Description Logistic Regression Tabular Baseline linear classifier Random Forest Tabular Ensemble of decision trees HMM Sequence Hidden Markov Model (hmmlearn) AutoGluon Tabular AutoML with model ensembling LSTM Sequence 2-layer LSTM neural network 1D-CNN Sequence 3-layer convolutional network Transformer Sequence Self-attention encoder <p>All models share a unified interface (<code>fit</code>, <code>predict</code>, <code>predict_proba</code>) and are benchmarked via k-fold cross-validation with bootstrap confidence intervals and paired statistical tests.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Realistic synthetic data with pitcher archetypes, fatigue, and game context</li> <li>7 prediction models spanning tabular and sequence architectures</li> <li>Comprehensive benchmarking with k-fold CV and bootstrap confidence intervals</li> <li>Ablation studies for feature importance, architecture, data scaling, and hyperparameters</li> <li>MLflow tracking for experiment management and comparison</li> <li>YAML-driven configuration for all settings</li> <li>CLI commands for data generation, training, benchmarking, and ablation</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Installation \u2014 Get up and running</li> <li>Quick Start \u2014 Generate data and train your first model</li> <li>Models Overview \u2014 Compare all 7 models</li> <li>CLI Reference \u2014 Command-line interface</li> <li>API Reference \u2014 Python API documentation</li> </ul>"},{"location":"cli-reference/","title":"CLI Reference","text":"<p>After installing the package (<code>pip install -e \".[all,dev]\"</code>), the following commands are available on your PATH.</p>"},{"location":"cli-reference/#pitch-generate","title":"pitch-generate","text":"<p>Generate synthetic baseball pitch datasets.</p> <pre><code>pitch-generate [OPTIONS]\n</code></pre> Flag Default Description <code>--num-games</code> 3000 Number of games to simulate <code>--at-bats</code> 35 At-bats per game <code>--seed</code> 42 Random seed <code>--output-dir</code> <code>data</code> Output directory for CSV files <p>Example:</p> <pre><code># Default: 3000 games, ~384K pitch rows\npitch-generate\n\n# Custom: smaller dataset\npitch-generate --num-games 500 --at-bats 30 --output-dir ./my-data\n</code></pre> <p>Output files:</p> <ul> <li><code>&lt;output-dir&gt;/baseball_pitch_data.csv</code> \u2014 Main pitch dataset</li> <li><code>&lt;output-dir&gt;/synthetic_pitch_sequences.csv</code> \u2014 HMM training sequences</li> </ul>"},{"location":"cli-reference/#pitch-train","title":"pitch-train","text":"<p>Train a single model.</p> <pre><code>pitch-train --model MODEL [OPTIONS]\n</code></pre> Flag Default Description <code>--model</code> (required) Model name from registry <code>--config</code> (auto-detected) Path to model config YAML <p>Available models: <code>logistic_regression</code>, <code>random_forest</code>, <code>hmm</code>, <code>autogluon</code>, <code>lstm</code>, <code>cnn1d</code>, <code>transformer</code></p> <p>Example:</p> <pre><code># Train with default config\npitch-train --model lstm\n\n# Train with custom config\npitch-train --model lstm --config configs/models/lstm.yaml\n</code></pre>"},{"location":"cli-reference/#pitch-benchmark","title":"pitch-benchmark","text":"<p>Run the full benchmark suite across all models.</p> <pre><code>pitch-benchmark [OPTIONS]\n</code></pre> Flag Default Description <code>--config</code> (auto-detected) Path to benchmark config YAML <p>Example:</p> <pre><code># Run all 7 models through 5-fold CV\npitch-benchmark\n\n# Custom benchmark config\npitch-benchmark --config my_benchmark.yaml\n</code></pre>"},{"location":"cli-reference/#pitch-ablation","title":"pitch-ablation","text":"<p>Run ablation studies on a specific model.</p> <pre><code>pitch-ablation --type TYPE [OPTIONS]\n</code></pre> Flag Default Description <code>--type</code> (required) Ablation type <code>--model</code> (from config) Model to ablate <p>Ablation types: <code>feature</code>, <code>architecture</code>, <code>data</code>, <code>hyperparam</code></p> <p>Example:</p> <pre><code># Feature importance ablation\npitch-ablation --type feature --model lstm\n\n# Data scaling ablation\npitch-ablation --type data --model transformer\n</code></pre>"},{"location":"cli-reference/#make-targets","title":"Make Targets","text":"<p>The Makefile provides shortcuts for common commands:</p> Target Command <code>make install</code> <code>pip install -e \".[all,dev]\"</code> <code>make data</code> <code>pitch-generate</code> <code>make train MODEL=lstm</code> <code>pitch-train --model lstm</code> <code>make benchmark</code> <code>pitch-benchmark</code> <code>make ablation TYPE=feature</code> <code>pitch-ablation --type feature</code> <code>make mlflow</code> <code>mlflow ui --backend-store-uri experiments</code> <code>make test</code> <code>pytest tests/</code> <code>make docs</code> <code>mkdocs build --strict</code> <code>make docs-serve</code> <code>mkdocs serve</code> <code>make clean</code> Remove build artifacts"},{"location":"configuration/","title":"Configuration","text":"<p>All settings are driven by YAML config files. Bundled configs ship with the package in <code>src/pitch_sequencing/configs/</code> and are mirrored in <code>configs/</code> for development.</p>"},{"location":"configuration/#data-configuration","title":"Data Configuration","text":"<pre><code># configs/data.yaml\ndata_path: data/baseball_pitch_data.csv\nhmm_data_path: data/synthetic_pitch_sequences.csv\ntarget_col: PitchType\noutcome_col: Outcome\n\ntest_size: 0.2\nn_folds: 5\nrandom_state: 42\nwindow_size: 8\n\ntabular_features:\n  - Balls\n  - Strikes\n  - PitcherType\n  - PreviousPitchType\n  - PitchNumber\n  - AtBatNumber\n  - RunnersOn\n  - ScoreDiff\n\nsequence_features:\n  - Balls\n  - Strikes\n  - PitchType_enc\n  - Outcome_enc\n  - PitcherType_enc\n  - PitchNumber\n  - AtBatNumber\n  - RunnersOn\n  - ScoreDiff\n  - PreviousPitchType_enc\n\ncategorical_features:\n  - PitchType\n  - Outcome\n  - PitcherType\n  - PreviousPitchType\n\nnumerical_features:\n  - PitchNumber\n  - AtBatNumber\n  - RunnersOn\n  - ScoreDiff\n</code></pre>"},{"location":"configuration/#benchmark-configuration","title":"Benchmark Configuration","text":"<pre><code># configs/benchmark.yaml\nexperiment_name: pitch_benchmark\nmodels:\n  - logistic_regression\n  - random_forest\n  - hmm\n  - autogluon\n  - lstm\n  - cnn1d\n  - transformer\nn_folds: 5\nmetrics:\n  - accuracy\n  - balanced_accuracy\n  - macro_f1\n  - log_loss\n</code></pre>"},{"location":"configuration/#model-configurations","title":"Model Configurations","text":""},{"location":"configuration/#logistic-regression","title":"Logistic Regression","text":"<pre><code># configs/models/logistic.yaml\nmodel_type: logistic_regression\nC: 1.0\npenalty: l2\nclass_weight: balanced\nmax_iter: 1000\n</code></pre>"},{"location":"configuration/#random-forest","title":"Random Forest","text":"<pre><code># configs/models/random_forest.yaml\nmodel_type: random_forest\nn_estimators: 200\nmax_depth: 15\nrandom_state: 42\n</code></pre>"},{"location":"configuration/#hmm","title":"HMM","text":"<pre><code># configs/models/hmm.yaml\nmodel_type: hmm\nmin_components: 1\nmax_components: 8\nn_iter: 100\n</code></pre>"},{"location":"configuration/#autogluon","title":"AutoGluon","text":"<pre><code># configs/models/autogluon.yaml\nmodel_type: autogluon\npreset: good_quality\ntime_limit: null\nmodels_dir: autogluon_pitchtype_models\n</code></pre>"},{"location":"configuration/#lstm","title":"LSTM","text":"<pre><code># configs/models/lstm.yaml\nmodel_type: lstm\nhidden_size: 64\nnum_layers: 2\ndropout: 0.3\nepochs: 20\nlearning_rate: 0.001\nbatch_size: 256\n</code></pre>"},{"location":"configuration/#1d-cnn","title":"1D-CNN","text":"<pre><code># configs/models/cnn1d.yaml\nmodel_type: cnn1d\nfilters: [64, 128, 64]\nkernel_size: 3\ndropout: 0.3\nepochs: 20\nlearning_rate: 0.001\nbatch_size: 256\n</code></pre>"},{"location":"configuration/#transformer","title":"Transformer","text":"<pre><code># configs/models/transformer.yaml\nmodel_type: transformer\nd_model: 64\nnhead: 4\nnum_layers: 2\ndropout: 0.3\nepochs: 20\nlearning_rate: 0.001\nbatch_size: 256\n</code></pre>"},{"location":"configuration/#ablation-configuration","title":"Ablation Configuration","text":"<pre><code># configs/ablation.yaml\nfeature_ablation:\n  model: lstm\n  features_to_drop:\n    - Balls\n    - Strikes\n    - PitcherType_enc\n    - PreviousPitchType_enc\n\ndata_ablation:\n  model: lstm\n  fractions: [0.1, 0.25, 0.5, 0.75, 1.0]\n\narchitecture_ablation:\n  model: lstm\n  variants:\n    - hidden_size: 32\n    - hidden_size: 64\n    - hidden_size: 128\n\nhyperparam_ablation:\n  model: lstm\n  params:\n    learning_rate: [0.0001, 0.001, 0.01]\n    dropout: [0.1, 0.3, 0.5]\n</code></pre>"},{"location":"configuration/#path-resolution","title":"Path Resolution","text":"<p>Configs are resolved using <code>pitch_sequencing.paths</code>:</p> <pre><code>from pitch_sequencing.paths import get_default_config, get_default_data_dir\n\n# Get bundled config path\nconfig_path = get_default_config(\"data.yaml\")\nmodel_config = get_default_config(\"models/lstm.yaml\")\n\n# Get default data directory\ndata_dir = get_default_data_dir()\n</code></pre> <p>The path resolution prefers local <code>./configs/</code> and <code>./data/</code> directories (for development) and falls back to bundled package resources (for installed usage).</p>"},{"location":"notebooks/","title":"Notebooks","text":"<p>Original exploratory Jupyter notebooks are preserved in <code>notebooks/</code> and can be run via Jupyter or Google Colab. They now import from the <code>pitch_sequencing</code> package.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":""},{"location":"notebooks/#1-baseball-pitch-sequence-simulator","title":"1. Baseball Pitch Sequence Simulator","text":"<p>File: <code>notebooks/Baseball_Pitch_Sequence_Simulator.ipynb</code></p> <p>Demonstrates the synthetic data generation pipeline, including pitcher archetypes, sequence strategies, count-dependent outcomes, fatigue modeling, and game context. Generates both the main pitch dataset and HMM training sequences.</p>"},{"location":"notebooks/#2-hmm-pitch-predictor","title":"2. HMM Pitch Predictor","text":"<p>File: <code>notebooks/HMM_Pitch_Predictor.ipynb</code></p> <p>Trains a Hidden Markov Model on synthetic pitch sequences. Sweeps the number of hidden states (1-8) and evaluates prediction accuracy. Shows transition matrices and emission probabilities.</p>"},{"location":"notebooks/#3-autogluon-baseball-pitch-prediction","title":"3. AutoGluon Baseball Pitch Prediction","text":"<p>File: <code>notebooks/AutoGluon_Baseball_Pitch_Prediction.ipynb</code></p> <p>Uses AutoGluon's TabularPredictor to predict the next pitch type from tabular features. Demonstrates automated model selection and ensembling.</p>"},{"location":"notebooks/#4-autogluon-baseball-pitch-outcome-prediction","title":"4. AutoGluon Baseball Pitch Outcome Prediction","text":"<p>File: <code>notebooks/AutoGluon_Baseball_Pitch_Outcome_Prediction.ipynb</code></p> <p>Predicts pitch outcomes (ball, strike, hit) rather than pitch types. Uses the same AutoGluon approach with outcome-specific features.</p>"},{"location":"notebooks/#5-lstm-pitch-predictor","title":"5. LSTM Pitch Predictor","text":"<p>File: <code>notebooks/LSTM_Pitch_Predictor.ipynb</code></p> <p>Trains a 2-layer LSTM on windowed pitch sequences. Includes data preprocessing, sequence creation, model training with early stopping, and evaluation with confusion matrices.</p>"},{"location":"notebooks/#running-notebooks","title":"Running Notebooks","text":""},{"location":"notebooks/#local-jupyter","title":"Local Jupyter","text":"<pre><code>source venv/bin/activate\npip install jupyter\njupyter notebook notebooks/\n</code></pre>"},{"location":"notebooks/#google-colab","title":"Google Colab","text":"<p>Upload any notebook to Google Colab and add this cell at the top:</p> <pre><code>!pip install git+https://github.com/jman4162/Baseball-Pitch-Sequence-Prediction.git\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>Auto-generated API documentation from source code docstrings.</p>"},{"location":"api/#top-level-package","title":"Top-Level Package","text":""},{"location":"api/#pitch_sequencing","title":"<code>pitch_sequencing</code>","text":"<p>Baseball pitch sequence prediction and analysis.</p>"},{"location":"api/#pitch_sequencing.__version__","title":"<code>__version__ = '0.1.0'</code>  <code>module-attribute</code>","text":""},{"location":"api/#pitch_sequencing.MODEL_REGISTRY","title":"<code>MODEL_REGISTRY = {'logistic_regression': LogisticRegressionModel, 'random_forest': RandomForestModel, 'hmm': HMMModel, 'autogluon': AutoGluonModel, 'lstm': LSTMModel, 'cnn1d': CNN1DModel, 'transformer': TransformerModel}</code>  <code>module-attribute</code>","text":""},{"location":"api/#pitch_sequencing.DataConfig","title":"<code>DataConfig</code>  <code>dataclass</code>","text":"Source code in <code>src/pitch_sequencing/config.py</code> <pre><code>@dataclass\nclass DataConfig:\n    data_path: str = field(default_factory=_default_data_path)\n    hmm_data_path: str = field(default_factory=_default_hmm_path)\n    target_col: str = \"PitchType\"\n    outcome_col: str = \"Outcome\"\n    test_size: float = 0.2\n    n_folds: int = 5\n    random_state: int = 42\n    window_size: int = 8\n    tabular_features: List[str] = field(default_factory=lambda: [\n        \"Balls\", \"Strikes\", \"PitcherType\", \"PitchNumber\",\n        \"AtBatNumber\", \"RunnersOn\", \"ScoreDiff\", \"PreviousPitchType\",\n    ])\n    sequence_features: List[str] = field(default_factory=lambda: [\n        \"PitchType_enc\", \"Balls\", \"Strikes\", \"PitcherType_enc\",\n        \"PitchNumber\", \"RunnersOn\", \"ScoreDiff\",\n    ])\n    categorical_features: List[str] = field(default_factory=lambda: [\n        \"PitchType\", \"PitcherType\", \"PreviousPitchType\", \"Outcome\",\n    ])\n    numerical_features: List[str] = field(default_factory=lambda: [\n        \"Balls\", \"Strikes\", \"PitchNumber\", \"AtBatNumber\", \"ScoreDiff\",\n    ])\n\n    @classmethod\n    def from_yaml(cls, path: str) -&gt; \"DataConfig\":\n        cfg = load_config(path)\n        return cls(**{k: v for k, v in cfg.items() if k in cls.__dataclass_fields__})\n</code></pre>"},{"location":"api/#pitch_sequencing.ModelConfig","title":"<code>ModelConfig</code>  <code>dataclass</code>","text":"Source code in <code>src/pitch_sequencing/config.py</code> <pre><code>@dataclass\nclass ModelConfig:\n    model_type: str = \"lstm\"\n    hyperparameters: Dict[str, Any] = field(default_factory=dict)\n\n    @classmethod\n    def from_yaml(cls, path: str) -&gt; \"ModelConfig\":\n        cfg = load_config(path)\n        model_type = cfg.pop(\"model_type\", \"lstm\")\n        return cls(model_type=model_type, hyperparameters=cfg)\n</code></pre>"},{"location":"api/#pitch_sequencing.get_model","title":"<code>get_model(name, config=None)</code>","text":"<p>Instantiate a model by registry name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Key in MODEL_REGISTRY (e.g. 'lstm', 'random_forest').</p> required <code>config</code> <p>Optional dict of hyperparameters.</p> <code>None</code> <p>Returns:</p> Type Description <p>Instance of the model class.</p> Source code in <code>src/pitch_sequencing/models/__init__.py</code> <pre><code>def get_model(name, config=None):\n    \"\"\"Instantiate a model by registry name.\n\n    Args:\n        name: Key in MODEL_REGISTRY (e.g. 'lstm', 'random_forest').\n        config: Optional dict of hyperparameters.\n\n    Returns:\n        Instance of the model class.\n    \"\"\"\n    if name not in MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model '{name}'. Available: {list(MODEL_REGISTRY.keys())}\")\n    return MODEL_REGISTRY[name](config)\n</code></pre>"},{"location":"api/#pitch_sequencing.load_pitch_data","title":"<code>load_pitch_data(path, filter_none_prev=True)</code>","text":"<p>Load the main pitch dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to baseball_pitch_data.csv.</p> required <code>filter_none_prev</code> <code>bool</code> <p>If True, drop rows where PreviousPitchType is 'None'.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with pitch data.</p> Source code in <code>src/pitch_sequencing/data/loader.py</code> <pre><code>def load_pitch_data(path: str, filter_none_prev: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Load the main pitch dataset.\n\n    Args:\n        path: Path to baseball_pitch_data.csv.\n        filter_none_prev: If True, drop rows where PreviousPitchType is 'None'.\n\n    Returns:\n        DataFrame with pitch data.\n    \"\"\"\n    df = pd.read_csv(path)\n    if filter_none_prev:\n        df = df[df[\"PreviousPitchType\"] != \"None\"].reset_index(drop=True)\n    return df\n</code></pre>"},{"location":"api/#pitch_sequencing.create_sequences","title":"<code>create_sequences(df, window_size=8, feature_cols=None, target_col='PitchType_enc')</code>","text":"<p>Create sliding-window sequences respecting game boundaries.</p> <p>Game boundaries are detected via PitchNumber resets (the raw column must be present or reconstructable). The function expects that categorical columns have already been encoded (e.g. PitchType_enc, PitcherType_enc).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with encoded features.</p> required <code>window_size</code> <code>int</code> <p>Number of previous timesteps per sample.</p> <code>8</code> <code>feature_cols</code> <code>Optional[List[str]]</code> <p>Columns to include as features in each timestep.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>Column to predict.</p> <code>'PitchType_enc'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>(X, y, game_starts) where X has shape (n_samples, window_size, n_features),</p> <code>ndarray</code> <p>y has shape (n_samples,), and game_starts lists the indices where new games start.</p> Source code in <code>src/pitch_sequencing/data/loader.py</code> <pre><code>def create_sequences(\n    df: pd.DataFrame,\n    window_size: int = 8,\n    feature_cols: Optional[List[str]] = None,\n    target_col: str = \"PitchType_enc\",\n) -&gt; Tuple[np.ndarray, np.ndarray, List[int]]:\n    \"\"\"Create sliding-window sequences respecting game boundaries.\n\n    Game boundaries are detected via PitchNumber resets (the raw column must\n    be present or reconstructable). The function expects that categorical\n    columns have already been encoded (e.g. PitchType_enc, PitcherType_enc).\n\n    Args:\n        df: DataFrame with encoded features.\n        window_size: Number of previous timesteps per sample.\n        feature_cols: Columns to include as features in each timestep.\n        target_col: Column to predict.\n\n    Returns:\n        (X, y, game_starts) where X has shape (n_samples, window_size, n_features),\n        y has shape (n_samples,), and game_starts lists the indices where new games start.\n    \"\"\"\n    if feature_cols is None:\n        feature_cols = [\n            \"PitchType_enc\", \"Balls\", \"Strikes\", \"PitcherType_enc\",\n            \"PitchNumber\", \"RunnersOn\", \"ScoreDiff\",\n        ]\n\n    features = df[feature_cols].values\n    targets = df[target_col].values\n\n    # Detect game boundaries using AtBatNumber resets (drops from high to low).\n    # Falls back to PitchNumber drops if AtBatNumber is not available.\n    if \"AtBatNumber_raw\" in df.columns:\n        boundary_col = df[\"AtBatNumber_raw\"].values\n    elif \"AtBatNumber\" in df.columns:\n        boundary_col = df[\"AtBatNumber\"].values\n    elif \"PitchNumber_raw\" in df.columns:\n        boundary_col = df[\"PitchNumber_raw\"].values\n    else:\n        boundary_col = df[\"PitchNumber\"].values\n    game_starts = set(np.where(np.diff(boundary_col, prepend=boundary_col[0] + 1) &lt; 0)[0])\n\n    X_sequences = []\n    y_targets = []\n\n    for i in range(window_size, len(features)):\n        window_range = range(i - window_size + 1, i + 1)\n        if any(idx in game_starts for idx in window_range):\n            continue\n        X_sequences.append(features[i - window_size:i])\n        y_targets.append(targets[i])\n\n    X = np.array(X_sequences, dtype=np.float32)\n    y = np.array(y_targets, dtype=np.int64)\n    return X, y, sorted(game_starts)\n</code></pre>"},{"location":"api/#pitch_sequencing.generate_dataset","title":"<code>generate_dataset(num_games=3000, at_bats_per_game=35, seed=42)</code>","text":"<p>Generate the main pitch dataset by simulating full games.</p> Source code in <code>src/pitch_sequencing/data/simulator.py</code> <pre><code>def generate_dataset(num_games: int = 3000, at_bats_per_game: int = 35, seed: int = 42) -&gt; pd.DataFrame:\n    \"\"\"Generate the main pitch dataset by simulating full games.\"\"\"\n    random.seed(seed)\n    pitcher_types = list(PITCHER_ARCHETYPES.keys())\n    data = []\n\n    for _ in range(num_games):\n        pitcher_type = random.choice(pitcher_types)\n        simulator = BaseballPitchSimulator(pitcher_type=pitcher_type)\n        score_diff = 0\n\n        for at_bat_num in range(1, at_bats_per_game + 1):\n            runners_on = random.random() &lt; 0.35\n            if random.random() &lt; 0.15:\n                score_diff += random.choice([-1, 1, 1, 2])\n            score_diff = max(min(score_diff, 8), -8)\n\n            at_bat = simulator.simulate_at_bat(runners_on=runners_on, score_diff=score_diff)\n            for item in at_bat[:-1]:\n                state, pitch_type, outcome = item\n                balls, strikes = state\n                data.append([\n                    balls, strikes, pitch_type, outcome,\n                    pitcher_type, simulator.pitch_count,\n                    at_bat_num, int(runners_on), score_diff,\n                ])\n\n    df = pd.DataFrame(data, columns=[\n        \"Balls\", \"Strikes\", \"PitchType\", \"Outcome\",\n        \"PitcherType\", \"PitchNumber\", \"AtBatNumber\",\n        \"RunnersOn\", \"ScoreDiff\",\n    ])\n    df[\"PreviousPitchType\"] = df[\"PitchType\"].shift(1).fillna(\"None\")\n    return df\n</code></pre>"},{"location":"api/cli/","title":"CLI","text":"<p>Command-line interface entry points.</p>"},{"location":"api/cli/#pitch_sequencing.cli","title":"<code>pitch_sequencing.cli</code>","text":"<p>CLI entry points for the pitch-sequencing package.</p> <p>These functions are referenced by <code>[project.scripts]</code> in pyproject.toml and can also be called from the thin wrapper scripts in <code>scripts/</code>.</p>"},{"location":"api/cli/#pitch_sequencing.cli.ablation_main","title":"<code>ablation_main()</code>","text":"<p>Run ablation studies.</p> Source code in <code>src/pitch_sequencing/cli.py</code> <pre><code>def ablation_main():\n    \"\"\"Run ablation studies.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run ablation studies\")\n    parser.add_argument(\"--type\", type=str, required=True,\n                        choices=[\"feature\", \"architecture\", \"data\", \"hyperparam\"],\n                        help=\"Type of ablation study\")\n    parser.add_argument(\"--model\", type=str, default=None, help=\"Model name (default: from config)\")\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Ablation config path\")\n    parser.add_argument(\"--data-config\", type=str, default=None, help=\"Data config path\")\n    args = parser.parse_args()\n\n    import matplotlib.pyplot as plt\n\n    from .config import AblationConfig, DataConfig\n    from .evaluation.ablation import AblationRunner\n    from .evaluation.visualization import plot_ablation_results\n\n    abl_config_path = args.config or str(get_default_config(\"ablation.yaml\"))\n    data_config_path = args.data_config or str(get_default_config(\"data.yaml\"))\n\n    abl_cfg = AblationConfig.from_yaml(abl_config_path)\n    data_cfg = DataConfig.from_yaml(data_config_path)\n\n    runner = AblationRunner(abl_cfg, data_cfg)\n\n    if args.type == \"feature\":\n        print(\"Running feature ablation...\")\n        results = runner.run_feature_ablation(args.model)\n    elif args.type == \"architecture\":\n        print(\"Running architecture ablation...\")\n        results = runner.run_architecture_ablation(args.model)\n    elif args.type == \"data\":\n        print(\"Running data size ablation...\")\n        results = runner.run_data_ablation(args.model)\n    elif args.type == \"hyperparam\":\n        print(\"Running hyperparameter sensitivity...\")\n        results = runner.run_hyperparameter_sensitivity(args.model)\n\n    if not results.empty:\n        print(\"\\nResults:\")\n        print(results.to_string(index=False))\n\n        os.makedirs(\"experiments\", exist_ok=True)\n        fig = plot_ablation_results(results, args.type)\n        output_path = f\"experiments/ablation_{args.type}.png\"\n        fig.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n        print(f\"\\nPlot saved to {output_path}\")\n        plt.close(fig)\n\n        results.to_csv(f\"experiments/ablation_{args.type}.csv\", index=False)\n        print(f\"Results saved to experiments/ablation_{args.type}.csv\")\n</code></pre>"},{"location":"api/cli/#pitch_sequencing.cli.benchmark_main","title":"<code>benchmark_main()</code>","text":"<p>Run the full benchmark suite across all models.</p> Source code in <code>src/pitch_sequencing/cli.py</code> <pre><code>def benchmark_main():\n    \"\"\"Run the full benchmark suite across all models.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Run pitch prediction benchmark\")\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Benchmark config path\")\n    parser.add_argument(\"--data-config\", type=str, default=None, help=\"Data config path\")\n    parser.add_argument(\"--models-dir\", type=str, default=None, help=\"Models config directory\")\n    args = parser.parse_args()\n\n    from .config import BenchmarkConfig, DataConfig\n    from .evaluation.benchmark import BenchmarkRunner\n\n    bench_config_path = args.config or str(get_default_config(\"benchmark.yaml\"))\n    data_config_path = args.data_config or str(get_default_config(\"data.yaml\"))\n    models_dir = args.models_dir or str(get_default_config(\"models\"))\n\n    bench_cfg = BenchmarkConfig.from_yaml(bench_config_path)\n    data_cfg = DataConfig.from_yaml(data_config_path)\n\n    runner = BenchmarkRunner(bench_cfg, data_cfg, models_config_dir=models_dir)\n    results = runner.run()\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"BENCHMARK RESULTS\")\n    print(\"=\" * 80)\n    print(runner.summary_table())\n\n    os.makedirs(\"experiments\", exist_ok=True)\n    results.to_csv(\"experiments/benchmark_results.csv\", index=False)\n    print(\"\\nResults saved to experiments/benchmark_results.csv\")\n</code></pre>"},{"location":"api/cli/#pitch_sequencing.cli.generate_main","title":"<code>generate_main()</code>","text":"<p>Generate synthetic baseball pitch datasets.</p> Source code in <code>src/pitch_sequencing/cli.py</code> <pre><code>def generate_main():\n    \"\"\"Generate synthetic baseball pitch datasets.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Generate synthetic baseball pitch data\")\n    parser.add_argument(\"--num-games\", type=int, default=3000, help=\"Number of games to simulate\")\n    parser.add_argument(\"--at-bats\", type=int, default=35, help=\"At-bats per game\")\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed\")\n    parser.add_argument(\"--output-dir\", type=str, default=\"data\", help=\"Output directory\")\n    args = parser.parse_args()\n\n    from .data.simulator import generate_dataset, generate_hmm_sequences\n\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    print(f\"Generating main dataset ({args.num_games} games, {args.at_bats} at-bats each)...\")\n    df = generate_dataset(num_games=args.num_games, at_bats_per_game=args.at_bats, seed=args.seed)\n    path = output_dir / \"baseball_pitch_data.csv\"\n    df.to_csv(path, index=False)\n    print(f\"  Saved {len(df)} rows to {path}\")\n    print(f\"  Pitch distribution:\\n{df['PitchType'].value_counts(normalize=True).round(3).to_string()}\")\n\n    print(\"\\nGenerating HMM sequences dataset...\")\n    hmm_df = generate_hmm_sequences(num_sequences=2500, sequence_length=100, seed=args.seed)\n    hmm_path = output_dir / \"synthetic_pitch_sequences.csv\"\n    hmm_df.to_csv(hmm_path, index=False)\n    print(f\"  Saved {len(hmm_df)} sequences to {hmm_path}\")\n\n    print(\"\\nDone!\")\n</code></pre>"},{"location":"api/cli/#pitch_sequencing.cli.train_main","title":"<code>train_main()</code>","text":"<p>Train a single pitch prediction model.</p> Source code in <code>src/pitch_sequencing/cli.py</code> <pre><code>def train_main():\n    \"\"\"Train a single pitch prediction model.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Train a single pitch prediction model\")\n    parser.add_argument(\"--model\", type=str, required=True, help=\"Model name (e.g. lstm, random_forest)\")\n    parser.add_argument(\"--config\", type=str, default=None, help=\"Path to model config YAML\")\n    parser.add_argument(\"--data-config\", type=str, default=None, help=\"Path to data config\")\n    args = parser.parse_args()\n\n    import mlflow\n    import numpy as np\n\n    from .config import DataConfig, load_config\n    from .data.loader import load_pitch_data, create_sequences, load_hmm_sequences\n    from .data.preprocessing import encode_categoricals, normalize_numericals\n    from .models import get_model\n    from .evaluation.metrics import compute_metrics\n\n    data_config_path = args.data_config or str(get_default_config(\"data.yaml\"))\n    data_cfg = DataConfig.from_yaml(data_config_path)\n\n    # Load model config\n    if args.config:\n        model_cfg = load_config(args.config)\n    else:\n        default_path = str(get_default_config(f\"models/{args.model.replace('logistic_regression', 'logistic')}.yaml\"))\n        if os.path.exists(default_path):\n            model_cfg = load_config(default_path)\n        else:\n            model_cfg = {}\n\n    # Load and prepare data\n    print(f\"Loading data from {data_cfg.data_path}...\")\n    df = load_pitch_data(data_cfg.data_path)\n    df, encoders = encode_categoricals(\n        df, [c for c in data_cfg.categorical_features if c in df.columns]\n    )\n    df, norm_stats = normalize_numericals(df, data_cfg.numerical_features)\n\n    model = get_model(args.model, model_cfg)\n    print(f\"Training {model.name} (type={model.model_type})...\")\n\n    if args.model == \"hmm\":\n        from sklearn.model_selection import train_test_split\n        hmm_flat, hmm_enc = load_hmm_sequences(data_cfg.hmm_data_path)\n        X_train, X_test = train_test_split(hmm_flat, test_size=data_cfg.test_size, random_state=data_cfg.random_state)\n        model.fit(X_train, X_train.flatten(), X_val=X_test, y_val=X_test.flatten())\n        y_pred = model.predict(X_test)\n        y_test = X_test.flatten()\n    elif model.model_type == \"sequence\":\n        X, y, _ = create_sequences(\n            df, window_size=data_cfg.window_size,\n            feature_cols=data_cfg.sequence_features,\n            target_col=f\"{data_cfg.target_col}_enc\",\n        )\n        split = int(len(X) * (1 - data_cfg.test_size))\n        X_train, X_test = X[:split], X[split:]\n        y_train, y_test = y[:split], y[split:]\n        model.fit(X_train, y_train, X_val=X_test, y_val=y_test)\n        y_pred = model.predict(X_test)\n    else:\n        tab_features = []\n        for col in data_cfg.tabular_features:\n            enc_col = f\"{col}_enc\"\n            if enc_col in df.columns:\n                tab_features.append(enc_col)\n            elif col in df.columns:\n                tab_features.append(col)\n        X = df[tab_features].values\n        y = df[f\"{data_cfg.target_col}_enc\"].values\n        split = int(len(X) * (1 - data_cfg.test_size))\n        X_train, X_test = X[:split], X[split:]\n        y_train, y_test = y[:split], y[split:]\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n\n    metrics = compute_metrics(y_test, y_pred)\n    print(f\"\\nResults for {model.name}:\")\n    print(f\"  Accuracy:          {metrics['accuracy']:.4f}\")\n    print(f\"  Balanced Accuracy: {metrics['balanced_accuracy']:.4f}\")\n    print(f\"  Macro F1:          {metrics['macro_f1']:.4f}\")\n    print(f\"  Macro Precision:   {metrics['macro_precision']:.4f}\")\n    print(f\"  Macro Recall:      {metrics['macro_recall']:.4f}\")\n\n    # Log to MLflow\n    mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n    mlflow.set_experiment(f\"train_{args.model}\")\n    with mlflow.start_run(run_name=f\"{args.model}_single\"):\n        mlflow.log_params(model.get_params())\n        for k, v in metrics.items():\n            if isinstance(v, (int, float)):\n                mlflow.log_metric(k, v)\n    print(\"\\nLogged to MLflow.\")\n</code></pre>"},{"location":"api/config/","title":"Config","text":"<p>Configuration loading and dataclasses.</p>"},{"location":"api/config/#pitch_sequencing.config","title":"<code>pitch_sequencing.config</code>","text":"<p>Configuration loading and dataclasses for the pitch sequencing package.</p>"},{"location":"api/config/#pitch_sequencing.config.load_config","title":"<code>load_config(path)</code>","text":"<p>Read a YAML config file and return a dict.</p> Source code in <code>src/pitch_sequencing/config.py</code> <pre><code>def load_config(path: str) -&gt; dict:\n    \"\"\"Read a YAML config file and return a dict.\"\"\"\n    with open(path) as f:\n        return yaml.safe_load(f)\n</code></pre>"},{"location":"api/paths/","title":"Paths","text":"<p>Config and data path resolution utilities.</p>"},{"location":"api/paths/#pitch_sequencing.paths","title":"<code>pitch_sequencing.paths</code>","text":"<p>Resolve default paths for configs and data shipped with the package.</p>"},{"location":"api/paths/#pitch_sequencing.paths.get_default_config","title":"<code>get_default_config(name)</code>","text":"<p>Return the path to a bundled config file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Config filename or relative path, e.g. <code>\"data.yaml\"</code> or   <code>\"models/lstm.yaml\"</code>. Passing <code>\"models\"</code> returns the   models config directory itself.</p> required Source code in <code>src/pitch_sequencing/paths.py</code> <pre><code>def get_default_config(name: str) -&gt; Path:\n    \"\"\"Return the path to a bundled config file.\n\n    Args:\n        name: Config filename or relative path, e.g. ``\"data.yaml\"`` or\n              ``\"models/lstm.yaml\"``. Passing ``\"models\"`` returns the\n              models config directory itself.\n    \"\"\"\n    ref = importlib.resources.files(\"pitch_sequencing\") / \"configs\" / name\n    # importlib.resources may return a Traversable; convert to a real Path\n    # so callers can pass it to open() / os.path.exists() / etc.\n    with importlib.resources.as_file(ref) as p:\n        return Path(p)\n</code></pre>"},{"location":"api/paths/#pitch_sequencing.paths.get_default_data_dir","title":"<code>get_default_data_dir()</code>","text":"<p>Return the default data directory.</p> <p>Preference order: 1. <code>./data</code> if it exists (repo checkout / development) 2. <code>~/pitch-sequencing-data/</code> (pip-installed usage)</p> Source code in <code>src/pitch_sequencing/paths.py</code> <pre><code>def get_default_data_dir() -&gt; Path:\n    \"\"\"Return the default data directory.\n\n    Preference order:\n    1. ``./data`` if it exists (repo checkout / development)\n    2. ``~/pitch-sequencing-data/`` (pip-installed usage)\n    \"\"\"\n    local = Path(\"data\")\n    if local.is_dir():\n        return local.resolve()\n    fallback = Path.home() / \"pitch-sequencing-data\"\n    fallback.mkdir(parents=True, exist_ok=True)\n    return fallback\n</code></pre>"},{"location":"api/data/loader/","title":"Data Loader","text":"<p>Data loading and sequence creation utilities.</p>"},{"location":"api/data/loader/#pitch_sequencing.data.loader","title":"<code>pitch_sequencing.data.loader</code>","text":"<p>Data loading utilities for pitch sequence prediction.</p>"},{"location":"api/data/loader/#pitch_sequencing.data.loader.create_sequences","title":"<code>create_sequences(df, window_size=8, feature_cols=None, target_col='PitchType_enc')</code>","text":"<p>Create sliding-window sequences respecting game boundaries.</p> <p>Game boundaries are detected via PitchNumber resets (the raw column must be present or reconstructable). The function expects that categorical columns have already been encoded (e.g. PitchType_enc, PitcherType_enc).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with encoded features.</p> required <code>window_size</code> <code>int</code> <p>Number of previous timesteps per sample.</p> <code>8</code> <code>feature_cols</code> <code>Optional[List[str]]</code> <p>Columns to include as features in each timestep.</p> <code>None</code> <code>target_col</code> <code>str</code> <p>Column to predict.</p> <code>'PitchType_enc'</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>(X, y, game_starts) where X has shape (n_samples, window_size, n_features),</p> <code>ndarray</code> <p>y has shape (n_samples,), and game_starts lists the indices where new games start.</p> Source code in <code>src/pitch_sequencing/data/loader.py</code> <pre><code>def create_sequences(\n    df: pd.DataFrame,\n    window_size: int = 8,\n    feature_cols: Optional[List[str]] = None,\n    target_col: str = \"PitchType_enc\",\n) -&gt; Tuple[np.ndarray, np.ndarray, List[int]]:\n    \"\"\"Create sliding-window sequences respecting game boundaries.\n\n    Game boundaries are detected via PitchNumber resets (the raw column must\n    be present or reconstructable). The function expects that categorical\n    columns have already been encoded (e.g. PitchType_enc, PitcherType_enc).\n\n    Args:\n        df: DataFrame with encoded features.\n        window_size: Number of previous timesteps per sample.\n        feature_cols: Columns to include as features in each timestep.\n        target_col: Column to predict.\n\n    Returns:\n        (X, y, game_starts) where X has shape (n_samples, window_size, n_features),\n        y has shape (n_samples,), and game_starts lists the indices where new games start.\n    \"\"\"\n    if feature_cols is None:\n        feature_cols = [\n            \"PitchType_enc\", \"Balls\", \"Strikes\", \"PitcherType_enc\",\n            \"PitchNumber\", \"RunnersOn\", \"ScoreDiff\",\n        ]\n\n    features = df[feature_cols].values\n    targets = df[target_col].values\n\n    # Detect game boundaries using AtBatNumber resets (drops from high to low).\n    # Falls back to PitchNumber drops if AtBatNumber is not available.\n    if \"AtBatNumber_raw\" in df.columns:\n        boundary_col = df[\"AtBatNumber_raw\"].values\n    elif \"AtBatNumber\" in df.columns:\n        boundary_col = df[\"AtBatNumber\"].values\n    elif \"PitchNumber_raw\" in df.columns:\n        boundary_col = df[\"PitchNumber_raw\"].values\n    else:\n        boundary_col = df[\"PitchNumber\"].values\n    game_starts = set(np.where(np.diff(boundary_col, prepend=boundary_col[0] + 1) &lt; 0)[0])\n\n    X_sequences = []\n    y_targets = []\n\n    for i in range(window_size, len(features)):\n        window_range = range(i - window_size + 1, i + 1)\n        if any(idx in game_starts for idx in window_range):\n            continue\n        X_sequences.append(features[i - window_size:i])\n        y_targets.append(targets[i])\n\n    X = np.array(X_sequences, dtype=np.float32)\n    y = np.array(y_targets, dtype=np.int64)\n    return X, y, sorted(game_starts)\n</code></pre>"},{"location":"api/data/loader/#pitch_sequencing.data.loader.load_hmm_sequences","title":"<code>load_hmm_sequences(path)</code>","text":"<p>Load the HMM synthetic pitch sequences dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to synthetic_pitch_sequences.csv.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>(flat_sequences, encoder) where flat_sequences is shape (n_total, 1)</p> <code>LabelEncoder</code> <p>of encoded pitch types, and encoder can invert labels.</p> Source code in <code>src/pitch_sequencing/data/loader.py</code> <pre><code>def load_hmm_sequences(path: str) -&gt; Tuple[np.ndarray, LabelEncoder]:\n    \"\"\"Load the HMM synthetic pitch sequences dataset.\n\n    Args:\n        path: Path to synthetic_pitch_sequences.csv.\n\n    Returns:\n        (flat_sequences, encoder) where flat_sequences is shape (n_total, 1)\n        of encoded pitch types, and encoder can invert labels.\n    \"\"\"\n    data = pd.read_csv(path).dropna()\n    encoder = LabelEncoder()\n    encoded = data.apply(encoder.fit_transform)\n    flat = encoded.values.flatten().reshape(-1, 1)\n    return flat, encoder\n</code></pre>"},{"location":"api/data/loader/#pitch_sequencing.data.loader.load_pitch_data","title":"<code>load_pitch_data(path, filter_none_prev=True)</code>","text":"<p>Load the main pitch dataset.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to baseball_pitch_data.csv.</p> required <code>filter_none_prev</code> <code>bool</code> <p>If True, drop rows where PreviousPitchType is 'None'.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with pitch data.</p> Source code in <code>src/pitch_sequencing/data/loader.py</code> <pre><code>def load_pitch_data(path: str, filter_none_prev: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Load the main pitch dataset.\n\n    Args:\n        path: Path to baseball_pitch_data.csv.\n        filter_none_prev: If True, drop rows where PreviousPitchType is 'None'.\n\n    Returns:\n        DataFrame with pitch data.\n    \"\"\"\n    df = pd.read_csv(path)\n    if filter_none_prev:\n        df = df[df[\"PreviousPitchType\"] != \"None\"].reset_index(drop=True)\n    return df\n</code></pre>"},{"location":"api/data/preprocessing/","title":"Data Preprocessing","text":"<p>Encoding, normalization, and train/test splitting.</p>"},{"location":"api/data/preprocessing/#pitch_sequencing.data.preprocessing","title":"<code>pitch_sequencing.data.preprocessing</code>","text":"<p>Preprocessing utilities for encoding, normalization, and splitting.</p>"},{"location":"api/data/preprocessing/#pitch_sequencing.data.preprocessing.create_splits","title":"<code>create_splits(X, y, test_size=0.2, n_folds=5, stratify=True, random_state=42, temporal=False)</code>","text":"<p>Create train/test splits: either a single split or k-fold CV.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature array.</p> required <code>y</code> <code>ndarray</code> <p>Target array.</p> required <code>test_size</code> <code>float</code> <p>Fraction for test set (single split mode).</p> <code>0.2</code> <code>n_folds</code> <code>int</code> <p>Number of CV folds. If 1, performs a single split.</p> <code>5</code> <code>stratify</code> <code>bool</code> <p>Whether to stratify by y.</p> <code>True</code> <code>random_state</code> <code>int</code> <p>Random seed.</p> <code>42</code> <code>temporal</code> <code>bool</code> <p>If True, use temporal (ordered) splits instead of random.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tuple[ndarray, ndarray]]</code> <p>List of (train_indices, test_indices) tuples.</p> Source code in <code>src/pitch_sequencing/data/preprocessing.py</code> <pre><code>def create_splits(\n    X: np.ndarray,\n    y: np.ndarray,\n    test_size: float = 0.2,\n    n_folds: int = 5,\n    stratify: bool = True,\n    random_state: int = 42,\n    temporal: bool = False,\n) -&gt; List[Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"Create train/test splits: either a single split or k-fold CV.\n\n    Args:\n        X: Feature array.\n        y: Target array.\n        test_size: Fraction for test set (single split mode).\n        n_folds: Number of CV folds. If 1, performs a single split.\n        stratify: Whether to stratify by y.\n        random_state: Random seed.\n        temporal: If True, use temporal (ordered) splits instead of random.\n\n    Returns:\n        List of (train_indices, test_indices) tuples.\n    \"\"\"\n    n = len(X)\n\n    if n_folds &lt;= 1:\n        if temporal:\n            split_idx = int(n * (1 - test_size))\n            return [(np.arange(split_idx), np.arange(split_idx, n))]\n        strat = y if stratify else None\n        train_idx, test_idx = train_test_split(\n            np.arange(n), test_size=test_size, stratify=strat, random_state=random_state\n        )\n        return [(train_idx, test_idx)]\n\n    if temporal:\n        fold_size = n // n_folds\n        folds = []\n        for i in range(n_folds):\n            test_start = i * fold_size\n            test_end = test_start + fold_size if i &lt; n_folds - 1 else n\n            test_idx = np.arange(test_start, test_end)\n            train_idx = np.concatenate([np.arange(0, test_start), np.arange(test_end, n)])\n            folds.append((train_idx, test_idx))\n        return folds\n\n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n    return [(train_idx, test_idx) for train_idx, test_idx in skf.split(X, y)]\n</code></pre>"},{"location":"api/data/preprocessing/#pitch_sequencing.data.preprocessing.encode_categoricals","title":"<code>encode_categoricals(df, columns, encoders=None)</code>","text":"<p>Label-encode categorical columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>columns</code> <code>List[str]</code> <p>Columns to encode.</p> required <code>encoders</code> <code>Optional[Dict[str, LabelEncoder]]</code> <p>Pre-fitted encoders to reuse (for test data).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>(df_encoded, encoders_dict) \u2014 DataFrame with new *_enc columns and</p> <code>Dict[str, LabelEncoder]</code> <p>the fitted encoders.</p> Source code in <code>src/pitch_sequencing/data/preprocessing.py</code> <pre><code>def encode_categoricals(\n    df: pd.DataFrame,\n    columns: List[str],\n    encoders: Optional[Dict[str, LabelEncoder]] = None,\n) -&gt; Tuple[pd.DataFrame, Dict[str, LabelEncoder]]:\n    \"\"\"Label-encode categorical columns.\n\n    Args:\n        df: Input DataFrame.\n        columns: Columns to encode.\n        encoders: Pre-fitted encoders to reuse (for test data).\n\n    Returns:\n        (df_encoded, encoders_dict) \u2014 DataFrame with new *_enc columns and\n        the fitted encoders.\n    \"\"\"\n    df = df.copy()\n    if encoders is None:\n        encoders = {}\n    for col in columns:\n        enc_col = f\"{col}_enc\"\n        if col not in encoders:\n            enc = LabelEncoder()\n            df[enc_col] = enc.fit_transform(df[col])\n            encoders[col] = enc\n        else:\n            df[enc_col] = encoders[col].transform(df[col])\n    return df, encoders\n</code></pre>"},{"location":"api/data/preprocessing/#pitch_sequencing.data.preprocessing.normalize_numericals","title":"<code>normalize_numericals(df, columns, stats=None)</code>","text":"<p>Z-score normalize numerical columns.</p> <p>Also stores the raw PitchNumber (before normalization) as PitchNumber_raw so that game-boundary detection still works downstream.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <code>columns</code> <code>List[str]</code> <p>Columns to normalize.</p> required <code>stats</code> <code>Optional[Dict[str, Tuple[float, float]]]</code> <p>Pre-computed (mean, std) per column (for test data).</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, Dict[str, Tuple[float, float]]]</code> <p>(df_normalized, stats_dict)</p> Source code in <code>src/pitch_sequencing/data/preprocessing.py</code> <pre><code>def normalize_numericals(\n    df: pd.DataFrame,\n    columns: List[str],\n    stats: Optional[Dict[str, Tuple[float, float]]] = None,\n) -&gt; Tuple[pd.DataFrame, Dict[str, Tuple[float, float]]]:\n    \"\"\"Z-score normalize numerical columns.\n\n    Also stores the raw PitchNumber (before normalization) as PitchNumber_raw\n    so that game-boundary detection still works downstream.\n\n    Args:\n        df: Input DataFrame.\n        columns: Columns to normalize.\n        stats: Pre-computed (mean, std) per column (for test data).\n\n    Returns:\n        (df_normalized, stats_dict)\n    \"\"\"\n    df = df.copy()\n    if stats is None:\n        stats = {}\n\n    # Preserve raw columns used for game boundary detection\n    for raw_col in [\"PitchNumber\", \"AtBatNumber\"]:\n        raw_name = f\"{raw_col}_raw\"\n        if raw_col in columns and raw_name not in df.columns:\n            df[raw_name] = df[raw_col].values.copy()\n\n    for col in columns:\n        if col not in stats:\n            mean = df[col].mean()\n            std = df[col].std() + 1e-8\n            stats[col] = (mean, std)\n        else:\n            mean, std = stats[col]\n        df[col] = (df[col] - mean) / std\n    return df, stats\n</code></pre>"},{"location":"api/data/simulator/","title":"Data Simulator","text":"<p>Synthetic baseball pitch data generation.</p>"},{"location":"api/data/simulator/#pitch_sequencing.data.simulator","title":"<code>pitch_sequencing.data.simulator</code>","text":"<p>Baseball pitch simulator with pitcher archetypes, sequence strategies, and fatigue.</p>"},{"location":"api/data/simulator/#pitch_sequencing.data.simulator.generate_dataset","title":"<code>generate_dataset(num_games=3000, at_bats_per_game=35, seed=42)</code>","text":"<p>Generate the main pitch dataset by simulating full games.</p> Source code in <code>src/pitch_sequencing/data/simulator.py</code> <pre><code>def generate_dataset(num_games: int = 3000, at_bats_per_game: int = 35, seed: int = 42) -&gt; pd.DataFrame:\n    \"\"\"Generate the main pitch dataset by simulating full games.\"\"\"\n    random.seed(seed)\n    pitcher_types = list(PITCHER_ARCHETYPES.keys())\n    data = []\n\n    for _ in range(num_games):\n        pitcher_type = random.choice(pitcher_types)\n        simulator = BaseballPitchSimulator(pitcher_type=pitcher_type)\n        score_diff = 0\n\n        for at_bat_num in range(1, at_bats_per_game + 1):\n            runners_on = random.random() &lt; 0.35\n            if random.random() &lt; 0.15:\n                score_diff += random.choice([-1, 1, 1, 2])\n            score_diff = max(min(score_diff, 8), -8)\n\n            at_bat = simulator.simulate_at_bat(runners_on=runners_on, score_diff=score_diff)\n            for item in at_bat[:-1]:\n                state, pitch_type, outcome = item\n                balls, strikes = state\n                data.append([\n                    balls, strikes, pitch_type, outcome,\n                    pitcher_type, simulator.pitch_count,\n                    at_bat_num, int(runners_on), score_diff,\n                ])\n\n    df = pd.DataFrame(data, columns=[\n        \"Balls\", \"Strikes\", \"PitchType\", \"Outcome\",\n        \"PitcherType\", \"PitchNumber\", \"AtBatNumber\",\n        \"RunnersOn\", \"ScoreDiff\",\n    ])\n    df[\"PreviousPitchType\"] = df[\"PitchType\"].shift(1).fillna(\"None\")\n    return df\n</code></pre>"},{"location":"api/data/simulator/#pitch_sequencing.data.simulator.generate_hmm_sequences","title":"<code>generate_hmm_sequences(num_sequences=2500, sequence_length=100, seed=42)</code>","text":"<p>Generate the HMM synthetic pitch sequences dataset.</p> Source code in <code>src/pitch_sequencing/data/simulator.py</code> <pre><code>def generate_hmm_sequences(\n    num_sequences: int = 2500,\n    sequence_length: int = 100,\n    seed: int = 42,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate the HMM synthetic pitch sequences dataset.\"\"\"\n    pitch_types = {0: \"Fastball\", 1: \"Curveball\", 2: \"Slider\", 3: \"Changeup\"}\n    num_pitches = len(pitch_types)\n\n    transition_matrix = np.array([\n        [0.15, 0.20, 0.30, 0.35],\n        [0.45, 0.10, 0.25, 0.20],\n        [0.35, 0.30, 0.10, 0.25],\n        [0.50, 0.15, 0.25, 0.10],\n    ])\n\n    np.random.seed(seed)\n    sequences = []\n    for _ in range(num_sequences):\n        start = np.random.randint(num_pitches)\n        seq = [start]\n        current = start\n        for _ in range(sequence_length - 1):\n            nxt = np.random.choice(num_pitches, p=transition_matrix[current])\n            seq.append(nxt)\n            current = nxt\n        sequences.append(seq)\n\n    df = pd.DataFrame(sequences, columns=[f\"Pitch_{i+1}\" for i in range(sequence_length)])\n    df.replace(pitch_types, inplace=True)\n    return df\n</code></pre>"},{"location":"api/evaluation/ablation/","title":"Ablation Runner","text":"<p>Ablation study runner for feature, architecture, data, and hyperparameter studies.</p>"},{"location":"api/evaluation/ablation/#pitch_sequencing.evaluation.ablation","title":"<code>pitch_sequencing.evaluation.ablation</code>","text":"<p>Ablation study runner for feature, architecture, data size, and hyperparameter studies.</p>"},{"location":"api/evaluation/ablation/#pitch_sequencing.evaluation.ablation.AblationRunner","title":"<code>AblationRunner</code>","text":"<p>Run ablation studies with MLflow logging.</p> Source code in <code>src/pitch_sequencing/evaluation/ablation.py</code> <pre><code>class AblationRunner:\n    \"\"\"Run ablation studies with MLflow logging.\"\"\"\n\n    def __init__(\n        self,\n        ablation_config: AblationConfig,\n        data_config: DataConfig,\n        models_config_dir: str = None,\n    ):\n        if models_config_dir is None:\n            models_config_dir = str(get_default_config(\"models\"))\n        self.cfg = ablation_config\n        self.data_cfg = data_config\n        self.models_config_dir = models_config_dir\n\n    def _load_and_prepare(self):\n        \"\"\"Load data and return (df, encoders, norm_stats).\"\"\"\n        df = load_pitch_data(self.data_cfg.data_path)\n        df, encoders = encode_categoricals(\n            df, [c for c in self.data_cfg.categorical_features if c in df.columns]\n        )\n        df, norm_stats = normalize_numericals(df, self.data_cfg.numerical_features)\n        return df, encoders, norm_stats\n\n    def _get_model_config(self, model_name: str) -&gt; dict:\n        config_path = os.path.join(\n            self.models_config_dir,\n            f\"{model_name.replace('logistic_regression', 'logistic')}.yaml\",\n        )\n        if not os.path.exists(config_path):\n            config_path = os.path.join(self.models_config_dir, f\"{model_name}.yaml\")\n        return load_config(config_path) if os.path.exists(config_path) else {}\n\n    def _train_and_evaluate(self, model_name, model_cfg, X, y, n_folds=3):\n        \"\"\"Train model with CV and return list of accuracy scores.\"\"\"\n        folds = create_splits(X, y, n_folds=n_folds, random_state=self.data_cfg.random_state)\n        scores = []\n        for train_idx, test_idx in folds:\n            model = get_model(model_name, model_cfg)\n            X_train, y_train = X[train_idx], y[train_idx]\n            X_test, y_test = X[test_idx], y[test_idx]\n            model.fit(X_train, y_train, X_val=X_test, y_val=y_test)\n            y_pred = model.predict(X_test)\n            scores.append(compute_metrics(y_test, y_pred)[\"accuracy\"])\n        return scores\n\n    def run_feature_ablation(self, model_name: Optional[str] = None) -&gt; pd.DataFrame:\n        \"\"\"Remove each feature group one-at-a-time, measure accuracy drop.\"\"\"\n        model_name = model_name or self.cfg.default_model\n        model_cfg = self._get_model_config(model_name)\n        df, encoders, norm_stats = self._load_and_prepare()\n\n        mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n        mlflow.set_experiment(f\"ablation_feature_{model_name}\")\n\n        # Determine base model type\n        temp_model = get_model(model_name, model_cfg)\n        is_sequence = temp_model.model_type == \"sequence\"\n\n        results = []\n        for group_name, features in self.cfg.feature_groups.items():\n            print(f\"  Feature group: {group_name}\")\n\n            if is_sequence:\n                # For sequence models, select which features to include\n                seq_features = []\n                for f in features:\n                    enc_col = f\"{f}_enc\"\n                    if enc_col in df.columns:\n                        seq_features.append(enc_col)\n                    elif f in df.columns:\n                        seq_features.append(f)\n                if not seq_features:\n                    continue\n                X, y, _ = create_sequences(\n                    df,\n                    window_size=self.data_cfg.window_size,\n                    feature_cols=seq_features,\n                    target_col=f\"{self.data_cfg.target_col}_enc\",\n                )\n            else:\n                tab_features = []\n                for f in features:\n                    enc_col = f\"{f}_enc\"\n                    if enc_col in df.columns:\n                        tab_features.append(enc_col)\n                    elif f in df.columns:\n                        tab_features.append(f)\n                if not tab_features:\n                    continue\n                X = df[tab_features].values\n                y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n            with mlflow.start_run(run_name=f\"feature_{group_name}\"):\n                scores = self._train_and_evaluate(model_name, model_cfg, X, y)\n                mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n                mlflow.log_metric(\"accuracy_mean\", mean)\n                mlflow.log_param(\"feature_group\", group_name)\n\n            results.append({\n                \"variant\": group_name,\n                \"accuracy\": mean,\n                \"ci_low\": ci_low,\n                \"ci_high\": ci_high,\n            })\n\n        return pd.DataFrame(results)\n\n    def run_architecture_ablation(self, model_name: Optional[str] = None) -&gt; pd.DataFrame:\n        \"\"\"For neural models, test architecture variants.\"\"\"\n        model_name = model_name or self.cfg.default_model\n        base_cfg = self._get_model_config(model_name)\n        df, encoders, norm_stats = self._load_and_prepare()\n\n        mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n        mlflow.set_experiment(f\"ablation_architecture_{model_name}\")\n\n        variants = self.cfg.architecture_variants.get(model_name, {})\n        if not variants:\n            print(f\"No architecture variants defined for {model_name}\")\n            return pd.DataFrame()\n\n        X, y, _ = create_sequences(\n            df,\n            window_size=self.data_cfg.window_size,\n            feature_cols=self.data_cfg.sequence_features,\n            target_col=f\"{self.data_cfg.target_col}_enc\",\n        )\n\n        results = []\n        for param_name, param_values in variants.items():\n            for val in param_values:\n                variant_cfg = dict(base_cfg)\n                variant_cfg[param_name] = val\n                variant_cfg[\"epochs\"] = min(variant_cfg.get(\"epochs\", 10), 10)\n                label = f\"{param_name}={val}\"\n                print(f\"  Architecture variant: {label}\")\n\n                with mlflow.start_run(run_name=f\"arch_{label}\"):\n                    scores = self._train_and_evaluate(model_name, variant_cfg, X, y)\n                    mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n                    mlflow.log_metric(\"accuracy_mean\", mean)\n                    mlflow.log_param(\"variant\", label)\n\n                results.append({\n                    \"variant\": label,\n                    \"accuracy\": mean,\n                    \"ci_low\": ci_low,\n                    \"ci_high\": ci_high,\n                })\n\n        return pd.DataFrame(results)\n\n    def run_data_ablation(self, model_name: Optional[str] = None) -&gt; pd.DataFrame:\n        \"\"\"Train on varying fractions of data, plot learning curves.\"\"\"\n        model_name = model_name or self.cfg.default_model\n        model_cfg = self._get_model_config(model_name)\n        df, encoders, norm_stats = self._load_and_prepare()\n\n        mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n        mlflow.set_experiment(f\"ablation_data_{model_name}\")\n\n        temp_model = get_model(model_name, model_cfg)\n        is_sequence = temp_model.model_type == \"sequence\"\n\n        if is_sequence:\n            X, y, _ = create_sequences(\n                df,\n                window_size=self.data_cfg.window_size,\n                feature_cols=self.data_cfg.sequence_features,\n                target_col=f\"{self.data_cfg.target_col}_enc\",\n            )\n        else:\n            tab_features = []\n            for col in self.data_cfg.tabular_features:\n                enc_col = f\"{col}_enc\"\n                if enc_col in df.columns:\n                    tab_features.append(enc_col)\n                elif col in df.columns:\n                    tab_features.append(col)\n            X = df[tab_features].values\n            y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n        results = []\n        for frac in self.cfg.data_fractions:\n            n_samples = int(len(X) * frac)\n            X_sub, y_sub = X[:n_samples], y[:n_samples]\n            label = f\"{frac*100:.0f}%\"\n            print(f\"  Data fraction: {label} ({n_samples} samples)\")\n\n            with mlflow.start_run(run_name=f\"data_{label}\"):\n                scores = self._train_and_evaluate(model_name, model_cfg, X_sub, y_sub)\n                mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n                mlflow.log_metric(\"accuracy_mean\", mean)\n                mlflow.log_param(\"data_fraction\", frac)\n\n            results.append({\n                \"variant\": label,\n                \"accuracy\": mean,\n                \"ci_low\": ci_low,\n                \"ci_high\": ci_high,\n                \"n_samples\": n_samples,\n            })\n\n        return pd.DataFrame(results)\n\n    def run_hyperparameter_sensitivity(\n        self, model_name: Optional[str] = None, param_name: Optional[str] = None, values: Optional[list] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Vary one hyperparameter, keep others at defaults.\"\"\"\n        model_name = model_name or self.cfg.default_model\n        base_cfg = self._get_model_config(model_name)\n        df, encoders, norm_stats = self._load_and_prepare()\n\n        mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n        mlflow.set_experiment(f\"ablation_hyperparam_{model_name}\")\n\n        if param_name is None and values is None:\n            # Run all configured sensitivity studies\n            all_results = []\n            for pname, pvalues in self.cfg.hyperparameter_sensitivity.items():\n                for val in pvalues:\n                    variant_cfg = dict(base_cfg)\n                    variant_cfg[pname] = val\n                    variant_cfg[\"epochs\"] = min(variant_cfg.get(\"epochs\", 10), 10)\n                    label = f\"{pname}={val}\"\n                    print(f\"  Hyperparam: {label}\")\n\n                    temp_model = get_model(model_name, variant_cfg)\n                    is_sequence = temp_model.model_type == \"sequence\"\n                    if is_sequence:\n                        ws = variant_cfg.get(\"window_size\", self.data_cfg.window_size)\n                        if pname == \"window_size\":\n                            ws = val\n                        X, y, _ = create_sequences(\n                            df, window_size=ws,\n                            feature_cols=self.data_cfg.sequence_features,\n                            target_col=f\"{self.data_cfg.target_col}_enc\",\n                        )\n                    else:\n                        tab_features = []\n                        for col in self.data_cfg.tabular_features:\n                            enc_col = f\"{col}_enc\"\n                            if enc_col in df.columns:\n                                tab_features.append(enc_col)\n                            elif col in df.columns:\n                                tab_features.append(col)\n                        X = df[tab_features].values\n                        y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n                    with mlflow.start_run(run_name=f\"hp_{label}\"):\n                        scores = self._train_and_evaluate(model_name, variant_cfg, X, y)\n                        mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n                        mlflow.log_metric(\"accuracy_mean\", mean)\n\n                    all_results.append({\n                        \"variant\": label,\n                        \"accuracy\": mean,\n                        \"ci_low\": ci_low,\n                        \"ci_high\": ci_high,\n                    })\n            return pd.DataFrame(all_results)\n\n        # Single param sweep\n        results = []\n        for val in values:\n            variant_cfg = dict(base_cfg)\n            variant_cfg[param_name] = val\n            label = f\"{param_name}={val}\"\n            print(f\"  Hyperparam: {label}\")\n\n            temp_model = get_model(model_name, variant_cfg)\n            is_sequence = temp_model.model_type == \"sequence\"\n            if is_sequence:\n                X, y, _ = create_sequences(\n                    df,\n                    window_size=self.data_cfg.window_size,\n                    feature_cols=self.data_cfg.sequence_features,\n                    target_col=f\"{self.data_cfg.target_col}_enc\",\n                )\n            else:\n                tab_features = []\n                for col in self.data_cfg.tabular_features:\n                    enc_col = f\"{col}_enc\"\n                    if enc_col in df.columns:\n                        tab_features.append(enc_col)\n                    elif col in df.columns:\n                        tab_features.append(col)\n                X = df[tab_features].values\n                y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n            with mlflow.start_run(run_name=f\"hp_{label}\"):\n                scores = self._train_and_evaluate(model_name, variant_cfg, X, y)\n                mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n                mlflow.log_metric(\"accuracy_mean\", mean)\n\n            results.append({\n                \"variant\": label,\n                \"accuracy\": mean,\n                \"ci_low\": ci_low,\n                \"ci_high\": ci_high,\n            })\n\n        return pd.DataFrame(results)\n</code></pre>"},{"location":"api/evaluation/ablation/#pitch_sequencing.evaluation.ablation.AblationRunner.run_architecture_ablation","title":"<code>run_architecture_ablation(model_name=None)</code>","text":"<p>For neural models, test architecture variants.</p> Source code in <code>src/pitch_sequencing/evaluation/ablation.py</code> <pre><code>def run_architecture_ablation(self, model_name: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"For neural models, test architecture variants.\"\"\"\n    model_name = model_name or self.cfg.default_model\n    base_cfg = self._get_model_config(model_name)\n    df, encoders, norm_stats = self._load_and_prepare()\n\n    mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n    mlflow.set_experiment(f\"ablation_architecture_{model_name}\")\n\n    variants = self.cfg.architecture_variants.get(model_name, {})\n    if not variants:\n        print(f\"No architecture variants defined for {model_name}\")\n        return pd.DataFrame()\n\n    X, y, _ = create_sequences(\n        df,\n        window_size=self.data_cfg.window_size,\n        feature_cols=self.data_cfg.sequence_features,\n        target_col=f\"{self.data_cfg.target_col}_enc\",\n    )\n\n    results = []\n    for param_name, param_values in variants.items():\n        for val in param_values:\n            variant_cfg = dict(base_cfg)\n            variant_cfg[param_name] = val\n            variant_cfg[\"epochs\"] = min(variant_cfg.get(\"epochs\", 10), 10)\n            label = f\"{param_name}={val}\"\n            print(f\"  Architecture variant: {label}\")\n\n            with mlflow.start_run(run_name=f\"arch_{label}\"):\n                scores = self._train_and_evaluate(model_name, variant_cfg, X, y)\n                mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n                mlflow.log_metric(\"accuracy_mean\", mean)\n                mlflow.log_param(\"variant\", label)\n\n            results.append({\n                \"variant\": label,\n                \"accuracy\": mean,\n                \"ci_low\": ci_low,\n                \"ci_high\": ci_high,\n            })\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"api/evaluation/ablation/#pitch_sequencing.evaluation.ablation.AblationRunner.run_data_ablation","title":"<code>run_data_ablation(model_name=None)</code>","text":"<p>Train on varying fractions of data, plot learning curves.</p> Source code in <code>src/pitch_sequencing/evaluation/ablation.py</code> <pre><code>def run_data_ablation(self, model_name: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"Train on varying fractions of data, plot learning curves.\"\"\"\n    model_name = model_name or self.cfg.default_model\n    model_cfg = self._get_model_config(model_name)\n    df, encoders, norm_stats = self._load_and_prepare()\n\n    mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n    mlflow.set_experiment(f\"ablation_data_{model_name}\")\n\n    temp_model = get_model(model_name, model_cfg)\n    is_sequence = temp_model.model_type == \"sequence\"\n\n    if is_sequence:\n        X, y, _ = create_sequences(\n            df,\n            window_size=self.data_cfg.window_size,\n            feature_cols=self.data_cfg.sequence_features,\n            target_col=f\"{self.data_cfg.target_col}_enc\",\n        )\n    else:\n        tab_features = []\n        for col in self.data_cfg.tabular_features:\n            enc_col = f\"{col}_enc\"\n            if enc_col in df.columns:\n                tab_features.append(enc_col)\n            elif col in df.columns:\n                tab_features.append(col)\n        X = df[tab_features].values\n        y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n    results = []\n    for frac in self.cfg.data_fractions:\n        n_samples = int(len(X) * frac)\n        X_sub, y_sub = X[:n_samples], y[:n_samples]\n        label = f\"{frac*100:.0f}%\"\n        print(f\"  Data fraction: {label} ({n_samples} samples)\")\n\n        with mlflow.start_run(run_name=f\"data_{label}\"):\n            scores = self._train_and_evaluate(model_name, model_cfg, X_sub, y_sub)\n            mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n            mlflow.log_metric(\"accuracy_mean\", mean)\n            mlflow.log_param(\"data_fraction\", frac)\n\n        results.append({\n            \"variant\": label,\n            \"accuracy\": mean,\n            \"ci_low\": ci_low,\n            \"ci_high\": ci_high,\n            \"n_samples\": n_samples,\n        })\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"api/evaluation/ablation/#pitch_sequencing.evaluation.ablation.AblationRunner.run_feature_ablation","title":"<code>run_feature_ablation(model_name=None)</code>","text":"<p>Remove each feature group one-at-a-time, measure accuracy drop.</p> Source code in <code>src/pitch_sequencing/evaluation/ablation.py</code> <pre><code>def run_feature_ablation(self, model_name: Optional[str] = None) -&gt; pd.DataFrame:\n    \"\"\"Remove each feature group one-at-a-time, measure accuracy drop.\"\"\"\n    model_name = model_name or self.cfg.default_model\n    model_cfg = self._get_model_config(model_name)\n    df, encoders, norm_stats = self._load_and_prepare()\n\n    mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n    mlflow.set_experiment(f\"ablation_feature_{model_name}\")\n\n    # Determine base model type\n    temp_model = get_model(model_name, model_cfg)\n    is_sequence = temp_model.model_type == \"sequence\"\n\n    results = []\n    for group_name, features in self.cfg.feature_groups.items():\n        print(f\"  Feature group: {group_name}\")\n\n        if is_sequence:\n            # For sequence models, select which features to include\n            seq_features = []\n            for f in features:\n                enc_col = f\"{f}_enc\"\n                if enc_col in df.columns:\n                    seq_features.append(enc_col)\n                elif f in df.columns:\n                    seq_features.append(f)\n            if not seq_features:\n                continue\n            X, y, _ = create_sequences(\n                df,\n                window_size=self.data_cfg.window_size,\n                feature_cols=seq_features,\n                target_col=f\"{self.data_cfg.target_col}_enc\",\n            )\n        else:\n            tab_features = []\n            for f in features:\n                enc_col = f\"{f}_enc\"\n                if enc_col in df.columns:\n                    tab_features.append(enc_col)\n                elif f in df.columns:\n                    tab_features.append(f)\n            if not tab_features:\n                continue\n            X = df[tab_features].values\n            y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n        with mlflow.start_run(run_name=f\"feature_{group_name}\"):\n            scores = self._train_and_evaluate(model_name, model_cfg, X, y)\n            mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n            mlflow.log_metric(\"accuracy_mean\", mean)\n            mlflow.log_param(\"feature_group\", group_name)\n\n        results.append({\n            \"variant\": group_name,\n            \"accuracy\": mean,\n            \"ci_low\": ci_low,\n            \"ci_high\": ci_high,\n        })\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"api/evaluation/ablation/#pitch_sequencing.evaluation.ablation.AblationRunner.run_hyperparameter_sensitivity","title":"<code>run_hyperparameter_sensitivity(model_name=None, param_name=None, values=None)</code>","text":"<p>Vary one hyperparameter, keep others at defaults.</p> Source code in <code>src/pitch_sequencing/evaluation/ablation.py</code> <pre><code>def run_hyperparameter_sensitivity(\n    self, model_name: Optional[str] = None, param_name: Optional[str] = None, values: Optional[list] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Vary one hyperparameter, keep others at defaults.\"\"\"\n    model_name = model_name or self.cfg.default_model\n    base_cfg = self._get_model_config(model_name)\n    df, encoders, norm_stats = self._load_and_prepare()\n\n    mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n    mlflow.set_experiment(f\"ablation_hyperparam_{model_name}\")\n\n    if param_name is None and values is None:\n        # Run all configured sensitivity studies\n        all_results = []\n        for pname, pvalues in self.cfg.hyperparameter_sensitivity.items():\n            for val in pvalues:\n                variant_cfg = dict(base_cfg)\n                variant_cfg[pname] = val\n                variant_cfg[\"epochs\"] = min(variant_cfg.get(\"epochs\", 10), 10)\n                label = f\"{pname}={val}\"\n                print(f\"  Hyperparam: {label}\")\n\n                temp_model = get_model(model_name, variant_cfg)\n                is_sequence = temp_model.model_type == \"sequence\"\n                if is_sequence:\n                    ws = variant_cfg.get(\"window_size\", self.data_cfg.window_size)\n                    if pname == \"window_size\":\n                        ws = val\n                    X, y, _ = create_sequences(\n                        df, window_size=ws,\n                        feature_cols=self.data_cfg.sequence_features,\n                        target_col=f\"{self.data_cfg.target_col}_enc\",\n                    )\n                else:\n                    tab_features = []\n                    for col in self.data_cfg.tabular_features:\n                        enc_col = f\"{col}_enc\"\n                        if enc_col in df.columns:\n                            tab_features.append(enc_col)\n                        elif col in df.columns:\n                            tab_features.append(col)\n                    X = df[tab_features].values\n                    y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n                with mlflow.start_run(run_name=f\"hp_{label}\"):\n                    scores = self._train_and_evaluate(model_name, variant_cfg, X, y)\n                    mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n                    mlflow.log_metric(\"accuracy_mean\", mean)\n\n                all_results.append({\n                    \"variant\": label,\n                    \"accuracy\": mean,\n                    \"ci_low\": ci_low,\n                    \"ci_high\": ci_high,\n                })\n        return pd.DataFrame(all_results)\n\n    # Single param sweep\n    results = []\n    for val in values:\n        variant_cfg = dict(base_cfg)\n        variant_cfg[param_name] = val\n        label = f\"{param_name}={val}\"\n        print(f\"  Hyperparam: {label}\")\n\n        temp_model = get_model(model_name, variant_cfg)\n        is_sequence = temp_model.model_type == \"sequence\"\n        if is_sequence:\n            X, y, _ = create_sequences(\n                df,\n                window_size=self.data_cfg.window_size,\n                feature_cols=self.data_cfg.sequence_features,\n                target_col=f\"{self.data_cfg.target_col}_enc\",\n            )\n        else:\n            tab_features = []\n            for col in self.data_cfg.tabular_features:\n                enc_col = f\"{col}_enc\"\n                if enc_col in df.columns:\n                    tab_features.append(enc_col)\n                elif col in df.columns:\n                    tab_features.append(col)\n            X = df[tab_features].values\n            y = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n        with mlflow.start_run(run_name=f\"hp_{label}\"):\n            scores = self._train_and_evaluate(model_name, variant_cfg, X, y)\n            mean, ci_low, ci_high = bootstrap_confidence_interval(scores)\n            mlflow.log_metric(\"accuracy_mean\", mean)\n\n        results.append({\n            \"variant\": label,\n            \"accuracy\": mean,\n            \"ci_low\": ci_low,\n            \"ci_high\": ci_high,\n        })\n\n    return pd.DataFrame(results)\n</code></pre>"},{"location":"api/evaluation/benchmark/","title":"Benchmark Runner","text":"<p>K-fold cross-validation benchmark across all models.</p>"},{"location":"api/evaluation/benchmark/#pitch_sequencing.evaluation.benchmark","title":"<code>pitch_sequencing.evaluation.benchmark</code>","text":"<p>Benchmark runner: k-fold cross-validation across all models.</p>"},{"location":"api/evaluation/benchmark/#pitch_sequencing.evaluation.benchmark.BenchmarkRunner","title":"<code>BenchmarkRunner</code>","text":"<p>Run all models through k-fold CV and produce comparison results.</p> Source code in <code>src/pitch_sequencing/evaluation/benchmark.py</code> <pre><code>class BenchmarkRunner:\n    \"\"\"Run all models through k-fold CV and produce comparison results.\"\"\"\n\n    def __init__(\n        self,\n        benchmark_config: BenchmarkConfig,\n        data_config: DataConfig,\n        models_config_dir: str = None,\n    ):\n        if models_config_dir is None:\n            models_config_dir = str(get_default_config(\"models\"))\n        self.cfg = benchmark_config\n        self.data_cfg = data_config\n        self.models_config_dir = models_config_dir\n        self._results: Optional[pd.DataFrame] = None\n\n    def run(self) -&gt; pd.DataFrame:\n        \"\"\"Run all models through k-fold CV, return results DataFrame.\"\"\"\n        # Set up MLflow\n        mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n        mlflow.set_experiment(self.cfg.experiment_name)\n\n        # Load and preprocess data (shared across all models)\n        df = load_pitch_data(self.data_cfg.data_path)\n        df, encoders = encode_categoricals(\n            df, [c for c in self.data_cfg.categorical_features if c in df.columns]\n        )\n        df, norm_stats = normalize_numericals(df, self.data_cfg.numerical_features)\n\n        # Prepare tabular data\n        tabular_feature_cols = []\n        for col in self.data_cfg.tabular_features:\n            enc_col = f\"{col}_enc\"\n            if enc_col in df.columns:\n                tabular_feature_cols.append(enc_col)\n            elif col in df.columns:\n                tabular_feature_cols.append(col)\n\n        X_tab = df[tabular_feature_cols].values\n        y_tab = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n        # Prepare sequence data\n        X_seq, y_seq, _ = create_sequences(\n            df,\n            window_size=self.data_cfg.window_size,\n            feature_cols=self.data_cfg.sequence_features,\n            target_col=f\"{self.data_cfg.target_col}_enc\",\n        )\n\n        # Prepare HMM data\n        hmm_flat, hmm_encoder = load_hmm_sequences(self.data_cfg.hmm_data_path)\n\n        # Create folds\n        tab_folds = create_splits(X_tab, y_tab, n_folds=self.cfg.n_folds,\n                                  random_state=self.data_cfg.random_state)\n        seq_folds = create_splits(X_seq, y_seq, n_folds=self.cfg.n_folds,\n                                  random_state=self.data_cfg.random_state)\n        hmm_folds = create_splits(hmm_flat, hmm_flat.flatten(), n_folds=self.cfg.n_folds,\n                                  random_state=self.data_cfg.random_state)\n\n        all_results = {}\n\n        for model_name in self.cfg.models:\n            print(f\"\\n{'='*60}\")\n            print(f\"Running: {model_name}\")\n            print(f\"{'='*60}\")\n\n            # Load model config\n            config_path = os.path.join(self.models_config_dir, f\"{model_name.replace('logistic_regression', 'logistic')}.yaml\")\n            if not os.path.exists(config_path):\n                config_path = os.path.join(self.models_config_dir, f\"{model_name}.yaml\")\n            model_cfg = load_config(config_path) if os.path.exists(config_path) else {}\n\n            fold_metrics = {m: [] for m in self.cfg.metrics}\n\n            # Determine data and folds\n            model_cls = get_model(model_name, model_cfg).__class__\n            temp_model = model_cls(model_cfg)\n\n            if model_name == \"hmm\":\n                data_X, data_y, folds = hmm_flat, hmm_flat.flatten(), hmm_folds\n            elif temp_model.model_type == \"sequence\":\n                data_X, data_y, folds = X_seq, y_seq, seq_folds\n            else:\n                data_X, data_y, folds = X_tab, y_tab, tab_folds\n\n            for fold_idx, (train_idx, test_idx) in enumerate(folds):\n                print(f\"  Fold {fold_idx + 1}/{self.cfg.n_folds}...\", end=\" \", flush=True)\n\n                model = get_model(model_name, model_cfg)\n\n                X_train, y_train = data_X[train_idx], data_y[train_idx]\n                X_test, y_test = data_X[test_idx], data_y[test_idx]\n\n                with mlflow.start_run(run_name=f\"{model_name}_fold{fold_idx}\"):\n                    mlflow.log_params(model.get_params())\n                    mlflow.log_param(\"model_name\", model_name)\n                    mlflow.log_param(\"fold\", fold_idx)\n\n                    model.fit(X_train, y_train, X_val=X_test, y_val=y_test)\n\n                    y_pred = model.predict(X_test)\n                    try:\n                        y_proba = model.predict_proba(X_test)\n                    except Exception:\n                        y_proba = None\n\n                    metrics = compute_metrics(y_test, y_pred, y_proba)\n\n                    for m in self.cfg.metrics:\n                        val = metrics.get(m, float(\"nan\"))\n                        fold_metrics[m].append(val)\n                        mlflow.log_metric(m, val)\n\n                print(f\"acc={metrics['accuracy']:.4f}\")\n\n            # Aggregate fold metrics\n            model_result = {\"model\": model_name}\n            for m in self.cfg.metrics:\n                scores = fold_metrics[m]\n                mean, ci_low, ci_high = bootstrap_confidence_interval(scores, self.cfg.confidence_level)\n                model_result[f\"{m}_mean\"] = mean\n                model_result[f\"{m}_ci_low\"] = ci_low\n                model_result[f\"{m}_ci_high\"] = ci_high\n                model_result[f\"{m}_scores\"] = scores\n\n            all_results[model_name] = model_result\n\n        self._results = pd.DataFrame([v for v in all_results.values()])\n\n        # Statistical tests between all pairs\n        if self.cfg.statistical_tests and len(self.cfg.models) &gt; 1:\n            print(\"\\nStatistical Tests (paired t-test on accuracy):\")\n            models = list(all_results.keys())\n            for i in range(len(models)):\n                for j in range(i + 1, len(models)):\n                    a_scores = all_results[models[i]][\"accuracy_scores\"]\n                    b_scores = all_results[models[j]][\"accuracy_scores\"]\n                    t_stat, p_val = paired_t_test(a_scores, b_scores)\n                    d = compute_effect_size(a_scores, b_scores)\n                    sig = \"*\" if p_val &lt; 0.05 else \"\"\n                    print(f\"  {models[i]} vs {models[j]}: t={t_stat:.3f}, p={p_val:.4f}{sig}, d={d:.3f}\")\n\n        return self._results\n\n    def summary_table(self) -&gt; str:\n        \"\"\"Return a markdown-formatted comparison table.\"\"\"\n        if self._results is None:\n            return \"No results yet. Run benchmark first.\"\n\n        lines = []\n        header_metrics = [m for m in self.cfg.metrics if m != \"log_loss\"]\n        header = \"| Model | \" + \" | \".join(m.replace(\"_\", \" \").title() for m in header_metrics) + \" |\"\n        sep = \"|\" + \"---|\" * (len(header_metrics) + 1)\n        lines.append(header)\n        lines.append(sep)\n\n        for _, row in self._results.iterrows():\n            cells = [row[\"model\"]]\n            for m in header_metrics:\n                mean = row.get(f\"{m}_mean\", float(\"nan\"))\n                ci_low = row.get(f\"{m}_ci_low\", float(\"nan\"))\n                ci_high = row.get(f\"{m}_ci_high\", float(\"nan\"))\n                half_width = (ci_high - ci_low) / 2\n                cells.append(f\"{mean:.3f} +/- {half_width:.3f}\")\n            lines.append(\"| \" + \" | \".join(cells) + \" |\")\n\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/evaluation/benchmark/#pitch_sequencing.evaluation.benchmark.BenchmarkRunner.run","title":"<code>run()</code>","text":"<p>Run all models through k-fold CV, return results DataFrame.</p> Source code in <code>src/pitch_sequencing/evaluation/benchmark.py</code> <pre><code>def run(self) -&gt; pd.DataFrame:\n    \"\"\"Run all models through k-fold CV, return results DataFrame.\"\"\"\n    # Set up MLflow\n    mlflow.set_tracking_uri(f\"file://{os.path.abspath('experiments')}\")\n    mlflow.set_experiment(self.cfg.experiment_name)\n\n    # Load and preprocess data (shared across all models)\n    df = load_pitch_data(self.data_cfg.data_path)\n    df, encoders = encode_categoricals(\n        df, [c for c in self.data_cfg.categorical_features if c in df.columns]\n    )\n    df, norm_stats = normalize_numericals(df, self.data_cfg.numerical_features)\n\n    # Prepare tabular data\n    tabular_feature_cols = []\n    for col in self.data_cfg.tabular_features:\n        enc_col = f\"{col}_enc\"\n        if enc_col in df.columns:\n            tabular_feature_cols.append(enc_col)\n        elif col in df.columns:\n            tabular_feature_cols.append(col)\n\n    X_tab = df[tabular_feature_cols].values\n    y_tab = df[f\"{self.data_cfg.target_col}_enc\"].values\n\n    # Prepare sequence data\n    X_seq, y_seq, _ = create_sequences(\n        df,\n        window_size=self.data_cfg.window_size,\n        feature_cols=self.data_cfg.sequence_features,\n        target_col=f\"{self.data_cfg.target_col}_enc\",\n    )\n\n    # Prepare HMM data\n    hmm_flat, hmm_encoder = load_hmm_sequences(self.data_cfg.hmm_data_path)\n\n    # Create folds\n    tab_folds = create_splits(X_tab, y_tab, n_folds=self.cfg.n_folds,\n                              random_state=self.data_cfg.random_state)\n    seq_folds = create_splits(X_seq, y_seq, n_folds=self.cfg.n_folds,\n                              random_state=self.data_cfg.random_state)\n    hmm_folds = create_splits(hmm_flat, hmm_flat.flatten(), n_folds=self.cfg.n_folds,\n                              random_state=self.data_cfg.random_state)\n\n    all_results = {}\n\n    for model_name in self.cfg.models:\n        print(f\"\\n{'='*60}\")\n        print(f\"Running: {model_name}\")\n        print(f\"{'='*60}\")\n\n        # Load model config\n        config_path = os.path.join(self.models_config_dir, f\"{model_name.replace('logistic_regression', 'logistic')}.yaml\")\n        if not os.path.exists(config_path):\n            config_path = os.path.join(self.models_config_dir, f\"{model_name}.yaml\")\n        model_cfg = load_config(config_path) if os.path.exists(config_path) else {}\n\n        fold_metrics = {m: [] for m in self.cfg.metrics}\n\n        # Determine data and folds\n        model_cls = get_model(model_name, model_cfg).__class__\n        temp_model = model_cls(model_cfg)\n\n        if model_name == \"hmm\":\n            data_X, data_y, folds = hmm_flat, hmm_flat.flatten(), hmm_folds\n        elif temp_model.model_type == \"sequence\":\n            data_X, data_y, folds = X_seq, y_seq, seq_folds\n        else:\n            data_X, data_y, folds = X_tab, y_tab, tab_folds\n\n        for fold_idx, (train_idx, test_idx) in enumerate(folds):\n            print(f\"  Fold {fold_idx + 1}/{self.cfg.n_folds}...\", end=\" \", flush=True)\n\n            model = get_model(model_name, model_cfg)\n\n            X_train, y_train = data_X[train_idx], data_y[train_idx]\n            X_test, y_test = data_X[test_idx], data_y[test_idx]\n\n            with mlflow.start_run(run_name=f\"{model_name}_fold{fold_idx}\"):\n                mlflow.log_params(model.get_params())\n                mlflow.log_param(\"model_name\", model_name)\n                mlflow.log_param(\"fold\", fold_idx)\n\n                model.fit(X_train, y_train, X_val=X_test, y_val=y_test)\n\n                y_pred = model.predict(X_test)\n                try:\n                    y_proba = model.predict_proba(X_test)\n                except Exception:\n                    y_proba = None\n\n                metrics = compute_metrics(y_test, y_pred, y_proba)\n\n                for m in self.cfg.metrics:\n                    val = metrics.get(m, float(\"nan\"))\n                    fold_metrics[m].append(val)\n                    mlflow.log_metric(m, val)\n\n            print(f\"acc={metrics['accuracy']:.4f}\")\n\n        # Aggregate fold metrics\n        model_result = {\"model\": model_name}\n        for m in self.cfg.metrics:\n            scores = fold_metrics[m]\n            mean, ci_low, ci_high = bootstrap_confidence_interval(scores, self.cfg.confidence_level)\n            model_result[f\"{m}_mean\"] = mean\n            model_result[f\"{m}_ci_low\"] = ci_low\n            model_result[f\"{m}_ci_high\"] = ci_high\n            model_result[f\"{m}_scores\"] = scores\n\n        all_results[model_name] = model_result\n\n    self._results = pd.DataFrame([v for v in all_results.values()])\n\n    # Statistical tests between all pairs\n    if self.cfg.statistical_tests and len(self.cfg.models) &gt; 1:\n        print(\"\\nStatistical Tests (paired t-test on accuracy):\")\n        models = list(all_results.keys())\n        for i in range(len(models)):\n            for j in range(i + 1, len(models)):\n                a_scores = all_results[models[i]][\"accuracy_scores\"]\n                b_scores = all_results[models[j]][\"accuracy_scores\"]\n                t_stat, p_val = paired_t_test(a_scores, b_scores)\n                d = compute_effect_size(a_scores, b_scores)\n                sig = \"*\" if p_val &lt; 0.05 else \"\"\n                print(f\"  {models[i]} vs {models[j]}: t={t_stat:.3f}, p={p_val:.4f}{sig}, d={d:.3f}\")\n\n    return self._results\n</code></pre>"},{"location":"api/evaluation/benchmark/#pitch_sequencing.evaluation.benchmark.BenchmarkRunner.summary_table","title":"<code>summary_table()</code>","text":"<p>Return a markdown-formatted comparison table.</p> Source code in <code>src/pitch_sequencing/evaluation/benchmark.py</code> <pre><code>def summary_table(self) -&gt; str:\n    \"\"\"Return a markdown-formatted comparison table.\"\"\"\n    if self._results is None:\n        return \"No results yet. Run benchmark first.\"\n\n    lines = []\n    header_metrics = [m for m in self.cfg.metrics if m != \"log_loss\"]\n    header = \"| Model | \" + \" | \".join(m.replace(\"_\", \" \").title() for m in header_metrics) + \" |\"\n    sep = \"|\" + \"---|\" * (len(header_metrics) + 1)\n    lines.append(header)\n    lines.append(sep)\n\n    for _, row in self._results.iterrows():\n        cells = [row[\"model\"]]\n        for m in header_metrics:\n            mean = row.get(f\"{m}_mean\", float(\"nan\"))\n            ci_low = row.get(f\"{m}_ci_low\", float(\"nan\"))\n            ci_high = row.get(f\"{m}_ci_high\", float(\"nan\"))\n            half_width = (ci_high - ci_low) / 2\n            cells.append(f\"{mean:.3f} +/- {half_width:.3f}\")\n        lines.append(\"| \" + \" | \".join(cells) + \" |\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/evaluation/metrics/","title":"Metrics","text":"<p>Evaluation metrics, bootstrap confidence intervals, and statistical tests.</p>"},{"location":"api/evaluation/metrics/#pitch_sequencing.evaluation.metrics","title":"<code>pitch_sequencing.evaluation.metrics</code>","text":"<p>Metrics computation, bootstrap confidence intervals, and statistical tests.</p>"},{"location":"api/evaluation/metrics/#pitch_sequencing.evaluation.metrics.bootstrap_confidence_interval","title":"<code>bootstrap_confidence_interval(scores, confidence=0.95, n_bootstrap=1000, seed=42)</code>","text":"<p>Compute bootstrap confidence interval for a list of scores.</p> <p>Parameters:</p> Name Type Description Default <code>scores</code> <code>List[float]</code> <p>List of metric values (e.g. per-fold accuracies).</p> required <code>confidence</code> <code>float</code> <p>Confidence level (default 0.95).</p> <code>0.95</code> <code>n_bootstrap</code> <code>int</code> <p>Number of bootstrap samples.</p> <code>1000</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>42</code> <p>Returns:</p> Type Description <code>Tuple[float, float, float]</code> <p>(mean, ci_low, ci_high)</p> Source code in <code>src/pitch_sequencing/evaluation/metrics.py</code> <pre><code>def bootstrap_confidence_interval(\n    scores: List[float],\n    confidence: float = 0.95,\n    n_bootstrap: int = 1000,\n    seed: int = 42,\n) -&gt; Tuple[float, float, float]:\n    \"\"\"Compute bootstrap confidence interval for a list of scores.\n\n    Args:\n        scores: List of metric values (e.g. per-fold accuracies).\n        confidence: Confidence level (default 0.95).\n        n_bootstrap: Number of bootstrap samples.\n        seed: Random seed.\n\n    Returns:\n        (mean, ci_low, ci_high)\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    arr = np.array(scores)\n    boot_means = []\n    for _ in range(n_bootstrap):\n        sample = rng.choice(arr, size=len(arr), replace=True)\n        boot_means.append(np.mean(sample))\n    boot_means = np.array(boot_means)\n    alpha = 1 - confidence\n    ci_low = np.percentile(boot_means, 100 * alpha / 2)\n    ci_high = np.percentile(boot_means, 100 * (1 - alpha / 2))\n    return float(np.mean(arr)), float(ci_low), float(ci_high)\n</code></pre>"},{"location":"api/evaluation/metrics/#pitch_sequencing.evaluation.metrics.compute_effect_size","title":"<code>compute_effect_size(scores_a, scores_b)</code>","text":"<p>Compute Cohen's d effect size between two score distributions.</p> Source code in <code>src/pitch_sequencing/evaluation/metrics.py</code> <pre><code>def compute_effect_size(\n    scores_a: List[float], scores_b: List[float]\n) -&gt; float:\n    \"\"\"Compute Cohen's d effect size between two score distributions.\"\"\"\n    a = np.array(scores_a)\n    b = np.array(scores_b)\n    pooled_std = np.sqrt((a.std(ddof=1) ** 2 + b.std(ddof=1) ** 2) / 2)\n    if pooled_std == 0:\n        return 0.0\n    return float((a.mean() - b.mean()) / pooled_std)\n</code></pre>"},{"location":"api/evaluation/metrics/#pitch_sequencing.evaluation.metrics.compute_metrics","title":"<code>compute_metrics(y_true, y_pred, y_proba=None, labels=None)</code>","text":"<p>Compute a comprehensive set of classification metrics.</p> <p>Returns dict with: accuracy, balanced_accuracy, macro_precision, macro_recall, macro_f1, per_class_precision, per_class_recall, per_class_f1, confusion_matrix, and optionally log_loss.</p> Source code in <code>src/pitch_sequencing/evaluation/metrics.py</code> <pre><code>def compute_metrics(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    y_proba: Optional[np.ndarray] = None,\n    labels: Optional[List] = None,\n) -&gt; Dict:\n    \"\"\"Compute a comprehensive set of classification metrics.\n\n    Returns dict with: accuracy, balanced_accuracy, macro_precision, macro_recall,\n    macro_f1, per_class_precision, per_class_recall, per_class_f1,\n    confusion_matrix, and optionally log_loss.\n    \"\"\"\n    result = {\n        \"accuracy\": accuracy_score(y_true, y_pred),\n        \"balanced_accuracy\": balanced_accuracy_score(y_true, y_pred),\n        \"macro_precision\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"macro_recall\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n        \"per_class_precision\": precision_score(y_true, y_pred, average=None, zero_division=0).tolist(),\n        \"per_class_recall\": recall_score(y_true, y_pred, average=None, zero_division=0).tolist(),\n        \"per_class_f1\": f1_score(y_true, y_pred, average=None, zero_division=0).tolist(),\n        \"confusion_matrix\": confusion_matrix(y_true, y_pred, labels=labels).tolist(),\n    }\n    if y_proba is not None:\n        try:\n            result[\"log_loss\"] = log_loss(y_true, y_proba, labels=labels)\n        except ValueError:\n            result[\"log_loss\"] = float(\"nan\")\n    return result\n</code></pre>"},{"location":"api/evaluation/metrics/#pitch_sequencing.evaluation.metrics.paired_t_test","title":"<code>paired_t_test(scores_a, scores_b)</code>","text":"<p>Paired t-test between two sets of fold scores.</p> <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>(t_statistic, p_value)</p> Source code in <code>src/pitch_sequencing/evaluation/metrics.py</code> <pre><code>def paired_t_test(\n    scores_a: List[float], scores_b: List[float]\n) -&gt; Tuple[float, float]:\n    \"\"\"Paired t-test between two sets of fold scores.\n\n    Returns:\n        (t_statistic, p_value)\n    \"\"\"\n    t_stat, p_val = stats.ttest_rel(scores_a, scores_b)\n    return float(t_stat), float(p_val)\n</code></pre>"},{"location":"api/evaluation/visualization/","title":"Visualization","text":"<p>Plotting utilities for confusion matrices, benchmark comparisons, and learning curves.</p>"},{"location":"api/evaluation/visualization/#pitch_sequencing.evaluation.visualization","title":"<code>pitch_sequencing.evaluation.visualization</code>","text":"<p>Visualization utilities for evaluation results.</p>"},{"location":"api/evaluation/visualization/#pitch_sequencing.evaluation.visualization.plot_ablation_results","title":"<code>plot_ablation_results(ablation_df, ablation_type='feature')</code>","text":"<p>Plot ablation study results.</p> <p>Parameters:</p> Name Type Description Default <code>ablation_df</code> <code>DataFrame</code> <p>DataFrame with 'variant' and 'accuracy' columns (and optionally 'ci_low', 'ci_high').</p> required <code>ablation_type</code> <code>str</code> <p>Type label for chart title.</p> <code>'feature'</code> Source code in <code>src/pitch_sequencing/evaluation/visualization.py</code> <pre><code>def plot_ablation_results(\n    ablation_df: pd.DataFrame,\n    ablation_type: str = \"feature\",\n) -&gt; plt.Figure:\n    \"\"\"Plot ablation study results.\n\n    Args:\n        ablation_df: DataFrame with 'variant' and 'accuracy' columns (and optionally 'ci_low', 'ci_high').\n        ablation_type: Type label for chart title.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    variants = ablation_df[\"variant\"]\n    means = ablation_df[\"accuracy\"]\n\n    if \"ci_low\" in ablation_df.columns:\n        errors = np.array([\n            means - ablation_df[\"ci_low\"],\n            ablation_df[\"ci_high\"] - means,\n        ])\n        ax.barh(range(len(variants)), means, xerr=errors, capsize=4, color=\"teal\", alpha=0.8)\n    else:\n        ax.barh(range(len(variants)), means, color=\"teal\", alpha=0.8)\n\n    ax.set_yticks(range(len(variants)))\n    ax.set_yticklabels(variants)\n    ax.set_xlabel(\"Accuracy\")\n    ax.set_title(f\"Ablation Study: {ablation_type.replace('_', ' ').title()}\")\n    ax.grid(axis=\"x\", alpha=0.3)\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/evaluation/visualization/#pitch_sequencing.evaluation.visualization.plot_benchmark_comparison","title":"<code>plot_benchmark_comparison(results_df, metric='accuracy')</code>","text":"<p>Plot grouped bar chart comparing models with CI error bars.</p> <p>Expects results_df to have columns: model, metric_mean, metric_ci_low, metric_ci_high.</p> Source code in <code>src/pitch_sequencing/evaluation/visualization.py</code> <pre><code>def plot_benchmark_comparison(\n    results_df: pd.DataFrame,\n    metric: str = \"accuracy\",\n) -&gt; plt.Figure:\n    \"\"\"Plot grouped bar chart comparing models with CI error bars.\n\n    Expects results_df to have columns: model, metric_mean, metric_ci_low, metric_ci_high.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(10, 6))\n    models = results_df[\"model\"]\n    means = results_df[f\"{metric}_mean\"]\n    ci_low = results_df[f\"{metric}_ci_low\"]\n    ci_high = results_df[f\"{metric}_ci_high\"]\n    errors = np.array([means - ci_low, ci_high - means])\n\n    bars = ax.bar(range(len(models)), means, yerr=errors, capsize=5, color=\"steelblue\", alpha=0.8)\n    ax.set_xticks(range(len(models)))\n    ax.set_xticklabels(models, rotation=30, ha=\"right\")\n    ax.set_ylabel(metric.replace(\"_\", \" \").title())\n    ax.set_title(f\"Model Comparison: {metric.replace('_', ' ').title()}\")\n    ax.grid(axis=\"y\", alpha=0.3)\n\n    for bar, mean in zip(bars, means):\n        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n                f\"{mean:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\")\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/evaluation/visualization/#pitch_sequencing.evaluation.visualization.plot_confusion_matrix","title":"<code>plot_confusion_matrix(y_true, y_pred, labels=None, title='Confusion Matrix')</code>","text":"<p>Plot a confusion matrix heatmap.</p> Source code in <code>src/pitch_sequencing/evaluation/visualization.py</code> <pre><code>def plot_confusion_matrix(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    labels: Optional[List[str]] = None,\n    title: str = \"Confusion Matrix\",\n) -&gt; plt.Figure:\n    \"\"\"Plot a confusion matrix heatmap.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 6))\n    cm = confusion_matrix(y_true, y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", ax=ax)\n    ax.set_title(title)\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/evaluation/visualization/#pitch_sequencing.evaluation.visualization.plot_feature_importance","title":"<code>plot_feature_importance(importance_dict)</code>","text":"<p>Plot horizontal bar chart of feature importance.</p> Source code in <code>src/pitch_sequencing/evaluation/visualization.py</code> <pre><code>def plot_feature_importance(importance_dict: Dict[str, float]) -&gt; plt.Figure:\n    \"\"\"Plot horizontal bar chart of feature importance.\"\"\"\n    fig, ax = plt.subplots(figsize=(8, 5))\n    features = list(importance_dict.keys())\n    values = list(importance_dict.values())\n    sorted_idx = np.argsort(values)\n    ax.barh([features[i] for i in sorted_idx], [values[i] for i in sorted_idx], color=\"coral\", alpha=0.8)\n    ax.set_xlabel(\"Importance (accuracy drop)\")\n    ax.set_title(\"Feature Importance (Leave-One-Out)\")\n    ax.grid(axis=\"x\", alpha=0.3)\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/evaluation/visualization/#pitch_sequencing.evaluation.visualization.plot_learning_curves","title":"<code>plot_learning_curves(history, title='')</code>","text":"<p>Plot train/val loss and accuracy curves.</p> Source code in <code>src/pitch_sequencing/evaluation/visualization.py</code> <pre><code>def plot_learning_curves(history: Dict[str, List[float]], title: str = \"\") -&gt; plt.Figure:\n    \"\"\"Plot train/val loss and accuracy curves.\"\"\"\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n    ax1.plot(history[\"train_losses\"], label=\"Train Loss\")\n    ax1.plot(history[\"val_losses\"], label=\"Val Loss\")\n    ax1.set_xlabel(\"Epoch\")\n    ax1.set_ylabel(\"Loss\")\n    ax1.set_title(f\"Loss Curves{f' \u2014 {title}' if title else ''}\")\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    ax2.plot(history[\"val_accuracies\"], label=\"Val Accuracy\", color=\"green\")\n    ax2.set_xlabel(\"Epoch\")\n    ax2.set_ylabel(\"Accuracy\")\n    ax2.set_title(f\"Validation Accuracy{f' \u2014 {title}' if title else ''}\")\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    return fig\n</code></pre>"},{"location":"api/models/","title":"Models Registry","text":"<p>Model registry and factory function.</p>"},{"location":"api/models/#pitch_sequencing.models","title":"<code>pitch_sequencing.models</code>","text":"<p>Model registry for all pitch prediction models.</p>"},{"location":"api/models/#pitch_sequencing.models.MODEL_REGISTRY","title":"<code>MODEL_REGISTRY = {'logistic_regression': LogisticRegressionModel, 'random_forest': RandomForestModel, 'hmm': HMMModel, 'autogluon': AutoGluonModel, 'lstm': LSTMModel, 'cnn1d': CNN1DModel, 'transformer': TransformerModel}</code>  <code>module-attribute</code>","text":""},{"location":"api/models/#pitch_sequencing.models.get_model","title":"<code>get_model(name, config=None)</code>","text":"<p>Instantiate a model by registry name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>Key in MODEL_REGISTRY (e.g. 'lstm', 'random_forest').</p> required <code>config</code> <p>Optional dict of hyperparameters.</p> <code>None</code> <p>Returns:</p> Type Description <p>Instance of the model class.</p> Source code in <code>src/pitch_sequencing/models/__init__.py</code> <pre><code>def get_model(name, config=None):\n    \"\"\"Instantiate a model by registry name.\n\n    Args:\n        name: Key in MODEL_REGISTRY (e.g. 'lstm', 'random_forest').\n        config: Optional dict of hyperparameters.\n\n    Returns:\n        Instance of the model class.\n    \"\"\"\n    if name not in MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model '{name}'. Available: {list(MODEL_REGISTRY.keys())}\")\n    return MODEL_REGISTRY[name](config)\n</code></pre>"},{"location":"api/models/autogluon/","title":"AutoGluon Model","text":"<p>AutoML model using AutoGluon TabularPredictor.</p>"},{"location":"api/models/autogluon/#pitch_sequencing.models.autogluon_model","title":"<code>pitch_sequencing.models.autogluon_model</code>","text":"<p>AutoGluon TabularPredictor wrapper.</p>"},{"location":"api/models/autogluon/#pitch_sequencing.models.autogluon_model.AutoGluonModel","title":"<code>AutoGluonModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>AutoGluon TabularPredictor for tabular pitch data.</p> Source code in <code>src/pitch_sequencing/models/autogluon_model.py</code> <pre><code>class AutoGluonModel(BaseModel):\n    \"\"\"AutoGluon TabularPredictor for tabular pitch data.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.preset = config.get(\"preset\", \"good_quality\")\n        self.time_limit = config.get(\"time_limit\", None)\n        self.models_dir = config.get(\"models_dir\", \"autogluon_pitchtype_models\")\n        self._predictor = None\n        self._label = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"AutoGluon\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"tabular\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        from autogluon.tabular import TabularDataset, TabularPredictor\n\n        self._label = y_train.name if hasattr(y_train, \"name\") else \"target\"\n        train_df = pd.DataFrame(X_train).copy()\n        train_df[self._label] = y_train.values if hasattr(y_train, \"values\") else y_train\n        train_data = TabularDataset(train_df)\n\n        fit_kwargs = {\"presets\": self.preset}\n        if self.time_limit is not None:\n            fit_kwargs[\"time_limit\"] = self.time_limit\n\n        self._predictor = TabularPredictor(\n            label=self._label, path=self.models_dir\n        ).fit(train_data, **fit_kwargs)\n\n    def predict(self, X) -&gt; np.ndarray:\n        from autogluon.tabular import TabularDataset\n\n        test_df = pd.DataFrame(X)\n        return self._predictor.predict(TabularDataset(test_df)).values\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        from autogluon.tabular import TabularDataset\n\n        test_df = pd.DataFrame(X)\n        proba = self._predictor.predict_proba(TabularDataset(test_df))\n        return proba.values\n\n    def get_params(self) -&gt; dict:\n        return {\"preset\": self.preset, \"time_limit\": self.time_limit}\n</code></pre>"},{"location":"api/models/base/","title":"BaseModel","text":"<p>Abstract base class for all models.</p>"},{"location":"api/models/base/#pitch_sequencing.models.base","title":"<code>pitch_sequencing.models.base</code>","text":"<p>Abstract base model for all pitch prediction models.</p>"},{"location":"api/models/base/#pitch_sequencing.models.base.BaseModel","title":"<code>BaseModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Unified interface for all pitch prediction models.</p> Source code in <code>src/pitch_sequencing/models/base.py</code> <pre><code>class BaseModel(ABC):\n    \"\"\"Unified interface for all pitch prediction models.\"\"\"\n\n    @property\n    @abstractmethod\n    def name(self) -&gt; str:\n        \"\"\"Human-readable model name for display.\"\"\"\n\n    @property\n    @abstractmethod\n    def model_type(self) -&gt; str:\n        \"\"\"'tabular' or 'sequence' \u2014 determines data format expected.\"\"\"\n\n    @abstractmethod\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        \"\"\"Train the model.\"\"\"\n\n    @abstractmethod\n    def predict(self, X) -&gt; np.ndarray:\n        \"\"\"Return predicted class labels.\"\"\"\n\n    @abstractmethod\n    def predict_proba(self, X) -&gt; np.ndarray:\n        \"\"\"Return class probabilities (n_samples x n_classes).\"\"\"\n\n    def get_params(self) -&gt; dict:\n        \"\"\"Return hyperparameters for logging.\"\"\"\n        return {}\n</code></pre>"},{"location":"api/models/base/#pitch_sequencing.models.base.BaseModel.model_type","title":"<code>model_type</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>'tabular' or 'sequence' \u2014 determines data format expected.</p>"},{"location":"api/models/base/#pitch_sequencing.models.base.BaseModel.name","title":"<code>name</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Human-readable model name for display.</p>"},{"location":"api/models/base/#pitch_sequencing.models.base.BaseModel.fit","title":"<code>fit(X_train, y_train, X_val=None, y_val=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Train the model.</p> Source code in <code>src/pitch_sequencing/models/base.py</code> <pre><code>@abstractmethod\ndef fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n    \"\"\"Train the model.\"\"\"\n</code></pre>"},{"location":"api/models/base/#pitch_sequencing.models.base.BaseModel.get_params","title":"<code>get_params()</code>","text":"<p>Return hyperparameters for logging.</p> Source code in <code>src/pitch_sequencing/models/base.py</code> <pre><code>def get_params(self) -&gt; dict:\n    \"\"\"Return hyperparameters for logging.\"\"\"\n    return {}\n</code></pre>"},{"location":"api/models/base/#pitch_sequencing.models.base.BaseModel.predict","title":"<code>predict(X)</code>  <code>abstractmethod</code>","text":"<p>Return predicted class labels.</p> Source code in <code>src/pitch_sequencing/models/base.py</code> <pre><code>@abstractmethod\ndef predict(self, X) -&gt; np.ndarray:\n    \"\"\"Return predicted class labels.\"\"\"\n</code></pre>"},{"location":"api/models/base/#pitch_sequencing.models.base.BaseModel.predict_proba","title":"<code>predict_proba(X)</code>  <code>abstractmethod</code>","text":"<p>Return class probabilities (n_samples x n_classes).</p> Source code in <code>src/pitch_sequencing/models/base.py</code> <pre><code>@abstractmethod\ndef predict_proba(self, X) -&gt; np.ndarray:\n    \"\"\"Return class probabilities (n_samples x n_classes).\"\"\"\n</code></pre>"},{"location":"api/models/baselines/","title":"Baselines","text":"<p>Logistic Regression and Random Forest model implementations.</p>"},{"location":"api/models/baselines/#pitch_sequencing.models.baselines","title":"<code>pitch_sequencing.models.baselines</code>","text":"<p>Baseline models: Logistic Regression and Random Forest.</p>"},{"location":"api/models/baselines/#pitch_sequencing.models.baselines.LogisticRegressionModel","title":"<code>LogisticRegressionModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Logistic Regression baseline for tabular pitch data.</p> Source code in <code>src/pitch_sequencing/models/baselines.py</code> <pre><code>class LogisticRegressionModel(BaseModel):\n    \"\"\"Logistic Regression baseline for tabular pitch data.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.C = config.get(\"C\", 1.0)\n        self.penalty = config.get(\"penalty\", \"l2\")\n        self.class_weight = config.get(\"class_weight\", \"balanced\")\n        self.max_iter = config.get(\"max_iter\", 1000)\n        self._model = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"Logistic Regression\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"tabular\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        self._model = LogisticRegression(\n            C=self.C,\n            penalty=self.penalty,\n            class_weight=self.class_weight,\n            max_iter=self.max_iter,\n            solver=\"lbfgs\",\n            multi_class=\"multinomial\",\n        )\n        self._model.fit(X_train, y_train)\n\n    def predict(self, X) -&gt; np.ndarray:\n        return self._model.predict(X)\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        return self._model.predict_proba(X)\n\n    def get_params(self) -&gt; dict:\n        return {\"C\": self.C, \"penalty\": self.penalty, \"class_weight\": self.class_weight}\n</code></pre>"},{"location":"api/models/baselines/#pitch_sequencing.models.baselines.RandomForestModel","title":"<code>RandomForestModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Random Forest baseline for tabular pitch data.</p> Source code in <code>src/pitch_sequencing/models/baselines.py</code> <pre><code>class RandomForestModel(BaseModel):\n    \"\"\"Random Forest baseline for tabular pitch data.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.n_estimators = config.get(\"n_estimators\", 200)\n        self.max_depth = config.get(\"max_depth\", 15)\n        self.min_samples_split = config.get(\"min_samples_split\", 5)\n        self.class_weight = config.get(\"class_weight\", \"balanced\")\n        self._model = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"Random Forest\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"tabular\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        self._model = RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            max_depth=self.max_depth,\n            min_samples_split=self.min_samples_split,\n            class_weight=self.class_weight,\n            random_state=42,\n            n_jobs=-1,\n        )\n        self._model.fit(X_train, y_train)\n\n    def predict(self, X) -&gt; np.ndarray:\n        return self._model.predict(X)\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        return self._model.predict_proba(X)\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"n_estimators\": self.n_estimators,\n            \"max_depth\": self.max_depth,\n            \"min_samples_split\": self.min_samples_split,\n        }\n</code></pre>"},{"location":"api/models/cnn1d/","title":"CNN1D Model","text":"<p>1D Convolutional neural network for pitch sequence prediction.</p>"},{"location":"api/models/cnn1d/#pitch_sequencing.models.cnn1d","title":"<code>pitch_sequencing.models.cnn1d</code>","text":"<p>1D-CNN model for pitch sequence prediction.</p>"},{"location":"api/models/cnn1d/#pitch_sequencing.models.cnn1d.CNN1DModel","title":"<code>CNN1DModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>1D-CNN wrapper implementing BaseModel interface.</p> Source code in <code>src/pitch_sequencing/models/cnn1d.py</code> <pre><code>class CNN1DModel(BaseModel):\n    \"\"\"1D-CNN wrapper implementing BaseModel interface.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.filters = config.get(\"filters\", [64, 128, 64])\n        self.kernel_size = config.get(\"kernel_size\", 3)\n        self.dropout = config.get(\"dropout\", 0.3)\n        self.epochs = config.get(\"epochs\", 30)\n        self.lr = config.get(\"learning_rate\", 0.001)\n        self.batch_size = config.get(\"batch_size\", 256)\n        self._model = None\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._history = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"1D-CNN\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        input_features = X_train.shape[2]\n        num_classes = len(np.unique(y_train))\n\n        self._model = PitchCNN1D(\n            input_features=input_features,\n            num_classes=num_classes,\n            filters=self.filters,\n            kernel_size=self.kernel_size,\n            dropout=self.dropout,\n        )\n\n        train_ds = PitchSequenceDataset(X_train, y_train)\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n\n        if X_val is not None and y_val is not None:\n            val_ds = PitchSequenceDataset(X_val, y_val)\n        else:\n            split = int(len(X_train) * 0.8)\n            val_ds = PitchSequenceDataset(X_train[split:], y_train[split:])\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n\n        self._history = train_torch_model(\n            self._model, train_loader, val_loader,\n            epochs=self.epochs, lr=self.lr, device=self._device,\n        )\n\n    def predict(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        preds, _ = predict_torch_model(self._model, loader, self._device)\n        return preds\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        _, probs = predict_torch_model(self._model, loader, self._device)\n        return probs\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"filters\": self.filters,\n            \"kernel_size\": self.kernel_size,\n            \"dropout\": self.dropout,\n            \"epochs\": self.epochs,\n            \"learning_rate\": self.lr,\n        }\n</code></pre>"},{"location":"api/models/cnn1d/#pitch_sequencing.models.cnn1d.PitchCNN1D","title":"<code>PitchCNN1D</code>","text":"<p>               Bases: <code>Module</code></p> <p>1D Convolutional network for pitch sequences.</p> Architecture <p>Input: (batch, window_size, n_features) -&gt; Transpose to (batch, n_features, window_size) for Conv1d -&gt; Conv1d(in, 64, k=3) + ReLU + BatchNorm -&gt; Conv1d(64, 128, k=3) + ReLU + BatchNorm -&gt; Conv1d(128, 64, k=3) + ReLU + BatchNorm -&gt; AdaptiveMaxPool1d(1) -&gt; squeeze -&gt; Dropout -&gt; Linear(64, num_classes)</p> Source code in <code>src/pitch_sequencing/models/cnn1d.py</code> <pre><code>class PitchCNN1D(nn.Module):\n    \"\"\"1D Convolutional network for pitch sequences.\n\n    Architecture:\n        Input: (batch, window_size, n_features)\n        -&gt; Transpose to (batch, n_features, window_size) for Conv1d\n        -&gt; Conv1d(in, 64, k=3) + ReLU + BatchNorm\n        -&gt; Conv1d(64, 128, k=3) + ReLU + BatchNorm\n        -&gt; Conv1d(128, 64, k=3) + ReLU + BatchNorm\n        -&gt; AdaptiveMaxPool1d(1) -&gt; squeeze\n        -&gt; Dropout -&gt; Linear(64, num_classes)\n    \"\"\"\n\n    def __init__(self, input_features, num_classes, filters=None, kernel_size=3, dropout=0.3):\n        super().__init__()\n        if filters is None:\n            filters = [64, 128, 64]\n\n        layers = []\n        in_channels = input_features\n        for out_channels in filters:\n            layers.extend([\n                nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2),\n                nn.ReLU(),\n                nn.BatchNorm1d(out_channels),\n            ])\n            in_channels = out_channels\n\n        self.conv_layers = nn.Sequential(*layers)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(filters[-1], num_classes)\n\n    def forward(self, x):\n        # x: (batch, seq_len, features) -&gt; (batch, features, seq_len)\n        x = x.transpose(1, 2)\n        x = self.conv_layers(x)\n        x = self.pool(x).squeeze(-1)\n        x = self.dropout(x)\n        return self.fc(x)\n</code></pre>"},{"location":"api/models/hmm/","title":"HMM Model","text":"<p>Hidden Markov Model implementation using hmmlearn.</p>"},{"location":"api/models/hmm/#pitch_sequencing.models.hmm_model","title":"<code>pitch_sequencing.models.hmm_model</code>","text":"<p>HMM model wrapper using hmmlearn CategoricalHMM.</p>"},{"location":"api/models/hmm/#pitch_sequencing.models.hmm_model.HMMModel","title":"<code>HMMModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Hidden Markov Model for pitch sequence prediction.</p> Source code in <code>src/pitch_sequencing/models/hmm_model.py</code> <pre><code>class HMMModel(BaseModel):\n    \"\"\"Hidden Markov Model for pitch sequence prediction.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.min_components = config.get(\"min_components\", 1)\n        self.max_components = config.get(\"max_components\", 8)\n        self.n_iter = config.get(\"n_iter\", 100)\n        self._model = None\n        self._best_n = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"HMM\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        \"\"\"Train HMM by sweeping n_components and picking best by validation accuracy.\n\n        For HMM, X_train is expected to be a flat 2D array of shape (n_samples, 1)\n        with encoded pitch types (the HMM uses its own flat encoding, not windowed).\n        y_train is the same flat array (self-supervised next-token prediction).\n        \"\"\"\n        from hmmlearn import hmm as hmmlearn_hmm\n\n        best_accuracy = 0\n        best_model = None\n\n        for n_components in range(self.min_components, self.max_components + 1):\n            model = hmmlearn_hmm.CategoricalHMM(\n                n_components=n_components,\n                n_iter=self.n_iter,\n                random_state=42,\n            )\n            model.fit(X_train)\n\n            if X_val is not None:\n                predicted = model.predict(X_val)\n                actual = X_val.flatten()\n                accuracy = np.mean(predicted == actual)\n            else:\n                predicted = model.predict(X_train)\n                actual = X_train.flatten()\n                accuracy = np.mean(predicted == actual)\n\n            if accuracy &gt; best_accuracy:\n                best_accuracy = accuracy\n                best_model = model\n                self._best_n = n_components\n\n        self._model = best_model\n\n    def predict(self, X) -&gt; np.ndarray:\n        return self._model.predict(X)\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        \"\"\"Return emission probabilities for each sample given predicted state.\"\"\"\n        states = self._model.predict(X)\n        emission = self._model.emissionprob_\n        return emission[states]\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"n_components\": self._best_n,\n            \"min_components\": self.min_components,\n            \"max_components\": self.max_components,\n            \"n_iter\": self.n_iter,\n        }\n</code></pre>"},{"location":"api/models/hmm/#pitch_sequencing.models.hmm_model.HMMModel.fit","title":"<code>fit(X_train, y_train, X_val=None, y_val=None, **kwargs)</code>","text":"<p>Train HMM by sweeping n_components and picking best by validation accuracy.</p> <p>For HMM, X_train is expected to be a flat 2D array of shape (n_samples, 1) with encoded pitch types (the HMM uses its own flat encoding, not windowed). y_train is the same flat array (self-supervised next-token prediction).</p> Source code in <code>src/pitch_sequencing/models/hmm_model.py</code> <pre><code>def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n    \"\"\"Train HMM by sweeping n_components and picking best by validation accuracy.\n\n    For HMM, X_train is expected to be a flat 2D array of shape (n_samples, 1)\n    with encoded pitch types (the HMM uses its own flat encoding, not windowed).\n    y_train is the same flat array (self-supervised next-token prediction).\n    \"\"\"\n    from hmmlearn import hmm as hmmlearn_hmm\n\n    best_accuracy = 0\n    best_model = None\n\n    for n_components in range(self.min_components, self.max_components + 1):\n        model = hmmlearn_hmm.CategoricalHMM(\n            n_components=n_components,\n            n_iter=self.n_iter,\n            random_state=42,\n        )\n        model.fit(X_train)\n\n        if X_val is not None:\n            predicted = model.predict(X_val)\n            actual = X_val.flatten()\n            accuracy = np.mean(predicted == actual)\n        else:\n            predicted = model.predict(X_train)\n            actual = X_train.flatten()\n            accuracy = np.mean(predicted == actual)\n\n        if accuracy &gt; best_accuracy:\n            best_accuracy = accuracy\n            best_model = model\n            self._best_n = n_components\n\n    self._model = best_model\n</code></pre>"},{"location":"api/models/hmm/#pitch_sequencing.models.hmm_model.HMMModel.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Return emission probabilities for each sample given predicted state.</p> Source code in <code>src/pitch_sequencing/models/hmm_model.py</code> <pre><code>def predict_proba(self, X) -&gt; np.ndarray:\n    \"\"\"Return emission probabilities for each sample given predicted state.\"\"\"\n    states = self._model.predict(X)\n    emission = self._model.emissionprob_\n    return emission[states]\n</code></pre>"},{"location":"api/models/lstm/","title":"LSTM Model","text":"<p>LSTM neural network for pitch sequence prediction.</p>"},{"location":"api/models/lstm/#pitch_sequencing.models.lstm","title":"<code>pitch_sequencing.models.lstm</code>","text":"<p>LSTM model for pitch sequence prediction.</p>"},{"location":"api/models/lstm/#pitch_sequencing.models.lstm.LSTMModel","title":"<code>LSTMModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>LSTM wrapper implementing BaseModel interface.</p> Source code in <code>src/pitch_sequencing/models/lstm.py</code> <pre><code>class LSTMModel(BaseModel):\n    \"\"\"LSTM wrapper implementing BaseModel interface.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.hidden_size = config.get(\"hidden_size\", 64)\n        self.num_layers = config.get(\"num_layers\", 2)\n        self.dropout = config.get(\"dropout\", 0.3)\n        self.epochs = config.get(\"epochs\", 20)\n        self.lr = config.get(\"learning_rate\", 0.001)\n        self.batch_size = config.get(\"batch_size\", 256)\n        self._model = None\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._num_classes = None\n        self._history = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"LSTM\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        input_size = X_train.shape[2]\n        self._num_classes = len(np.unique(y_train))\n\n        self._model = PitchPredictor(\n            input_size=input_size,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            num_classes=self._num_classes,\n            dropout=self.dropout,\n        )\n\n        train_ds = PitchSequenceDataset(X_train, y_train)\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n\n        if X_val is not None and y_val is not None:\n            val_ds = PitchSequenceDataset(X_val, y_val)\n        else:\n            # Use last 20% of training data as validation\n            split = int(len(X_train) * 0.8)\n            val_ds = PitchSequenceDataset(X_train[split:], y_train[split:])\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n\n        self._history = train_torch_model(\n            self._model, train_loader, val_loader,\n            epochs=self.epochs, lr=self.lr, device=self._device,\n        )\n\n    def predict(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        preds, _ = predict_torch_model(self._model, loader, self._device)\n        return preds\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        _, probs = predict_torch_model(self._model, loader, self._device)\n        return probs\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"hidden_size\": self.hidden_size,\n            \"num_layers\": self.num_layers,\n            \"dropout\": self.dropout,\n            \"epochs\": self.epochs,\n            \"learning_rate\": self.lr,\n            \"batch_size\": self.batch_size,\n        }\n</code></pre>"},{"location":"api/models/lstm/#pitch_sequencing.models.lstm.PitchPredictor","title":"<code>PitchPredictor</code>","text":"<p>               Bases: <code>Module</code></p> <p>2-layer LSTM for pitch type classification.</p> Source code in <code>src/pitch_sequencing/models/lstm.py</code> <pre><code>class PitchPredictor(nn.Module):\n    \"\"\"2-layer LSTM for pitch type classification.\"\"\"\n\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers &gt; 1 else 0,\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        _, (h_n, _) = self.lstm(x)\n        out = self.dropout(h_n[-1])\n        return self.fc(out)\n</code></pre>"},{"location":"api/models/torch_utils/","title":"Torch Utilities","text":"<p>Shared PyTorch training loop, dataset class, and prediction utilities.</p>"},{"location":"api/models/torch_utils/#pitch_sequencing.models.torch_utils","title":"<code>pitch_sequencing.models.torch_utils</code>","text":"<p>Shared PyTorch training utilities for sequence models.</p>"},{"location":"api/models/torch_utils/#pitch_sequencing.models.torch_utils.PitchSequenceDataset","title":"<code>PitchSequenceDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Wraps numpy arrays as a PyTorch Dataset.</p> Source code in <code>src/pitch_sequencing/models/torch_utils.py</code> <pre><code>class PitchSequenceDataset(Dataset):\n    \"\"\"Wraps numpy arrays as a PyTorch Dataset.\"\"\"\n\n    def __init__(self, sequences: np.ndarray, targets: np.ndarray):\n        self.sequences = torch.FloatTensor(sequences)\n        self.targets = torch.LongTensor(targets)\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        return self.sequences[idx], self.targets[idx]\n</code></pre>"},{"location":"api/models/torch_utils/#pitch_sequencing.models.torch_utils.predict_torch_model","title":"<code>predict_torch_model(model, data_loader, device=None)</code>","text":"<p>Run inference and return (predictions, probabilities).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>(predictions, probabilities) where predictions has shape (n,)</p> <code>ndarray</code> <p>and probabilities has shape (n, num_classes).</p> Source code in <code>src/pitch_sequencing/models/torch_utils.py</code> <pre><code>def predict_torch_model(\n    model: nn.Module,\n    data_loader: DataLoader,\n    device: Optional[torch.device] = None,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Run inference and return (predictions, probabilities).\n\n    Returns:\n        (predictions, probabilities) where predictions has shape (n,)\n        and probabilities has shape (n, num_classes).\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n\n    all_preds = []\n    all_probs = []\n    with torch.no_grad():\n        for batch_X, _ in data_loader:\n            batch_X = batch_X.to(device)\n            outputs = model(batch_X)\n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(probs, dim=1)\n            all_preds.append(preds.cpu().numpy())\n            all_probs.append(probs.cpu().numpy())\n\n    return np.concatenate(all_preds), np.concatenate(all_probs)\n</code></pre>"},{"location":"api/models/torch_utils/#pitch_sequencing.models.torch_utils.train_torch_model","title":"<code>train_torch_model(model, train_loader, val_loader, epochs=20, lr=0.001, patience=5, device=None)</code>","text":"<p>Train a PyTorch model with Adam, LR scheduling, early stopping, and gradient clipping.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary with train_losses, val_losses, val_accuracies.</p> Source code in <code>src/pitch_sequencing/models/torch_utils.py</code> <pre><code>def train_torch_model(\n    model: nn.Module,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    epochs: int = 20,\n    lr: float = 0.001,\n    patience: int = 5,\n    device: Optional[torch.device] = None,\n) -&gt; Dict:\n    \"\"\"Train a PyTorch model with Adam, LR scheduling, early stopping, and gradient clipping.\n\n    Returns:\n        Dictionary with train_losses, val_losses, val_accuracies.\n    \"\"\"\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n\n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n    best_val_loss = float(\"inf\")\n    best_state = None\n    wait = 0\n\n    for epoch in range(epochs):\n        # Train\n        model.train()\n        total_train_loss = 0\n        for batch_X, batch_y in train_loader:\n            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n            optimizer.zero_grad()\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_y)\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            total_train_loss += loss.item()\n\n        avg_train_loss = total_train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n\n        # Validate\n        model.eval()\n        total_val_loss = 0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for batch_X, batch_y in val_loader:\n                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n                outputs = model(batch_X)\n                loss = criterion(outputs, batch_y)\n                total_val_loss += loss.item()\n                preds = torch.argmax(outputs, dim=1)\n                correct += (preds == batch_y).sum().item()\n                total += batch_y.size(0)\n\n        avg_val_loss = total_val_loss / len(val_loader)\n        val_losses.append(avg_val_loss)\n        val_acc = correct / total\n        val_accuracies.append(val_acc)\n\n        scheduler.step(avg_val_loss)\n\n        # Early stopping\n        if avg_val_loss &lt; best_val_loss:\n            best_val_loss = avg_val_loss\n            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            wait = 0\n        else:\n            wait += 1\n            if wait &gt;= patience:\n                break\n\n    # Restore best model\n    if best_state is not None:\n        model.load_state_dict(best_state)\n    model = model.to(device)\n\n    return {\n        \"train_losses\": train_losses,\n        \"val_losses\": val_losses,\n        \"val_accuracies\": val_accuracies,\n    }\n</code></pre>"},{"location":"api/models/transformer/","title":"Transformer Model","text":"<p>Transformer encoder for pitch sequence prediction.</p>"},{"location":"api/models/transformer/#pitch_sequencing.models.transformer","title":"<code>pitch_sequencing.models.transformer</code>","text":"<p>Transformer model for pitch sequence prediction.</p>"},{"location":"api/models/transformer/#pitch_sequencing.models.transformer.PitchTransformer","title":"<code>PitchTransformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer encoder for pitch sequences.</p> Architecture <p>Input: (batch, window_size, n_features) -&gt; Linear(n_features, d_model) \u2014 input projection -&gt; + sinusoidal PositionalEncoding -&gt; TransformerEncoderLayer x num_layers -&gt; Mean pool across sequence dimension -&gt; Dropout -&gt; Linear(d_model, num_classes)</p> Source code in <code>src/pitch_sequencing/models/transformer.py</code> <pre><code>class PitchTransformer(nn.Module):\n    \"\"\"Transformer encoder for pitch sequences.\n\n    Architecture:\n        Input: (batch, window_size, n_features)\n        -&gt; Linear(n_features, d_model) \u2014 input projection\n        -&gt; + sinusoidal PositionalEncoding\n        -&gt; TransformerEncoderLayer x num_layers\n        -&gt; Mean pool across sequence dimension\n        -&gt; Dropout -&gt; Linear(d_model, num_classes)\n    \"\"\"\n\n    def __init__(\n        self,\n        input_features: int,\n        num_classes: int,\n        d_model: int = 64,\n        nhead: int = 4,\n        num_layers: int = 2,\n        dim_feedforward: int = 128,\n        dropout: float = 0.2,\n    ):\n        super().__init__()\n        self.input_proj = nn.Linear(input_features, d_model)\n        self.pos_encoder = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True,\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        x = self.input_proj(x)\n        x = self.pos_encoder(x)\n        x = self.transformer_encoder(x)\n        x = x.mean(dim=1)  # mean pool across sequence\n        x = self.dropout(x)\n        return self.fc(x)\n</code></pre>"},{"location":"api/models/transformer/#pitch_sequencing.models.transformer.PositionalEncoding","title":"<code>PositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sinusoidal positional encoding.</p> Source code in <code>src/pitch_sequencing/models/transformer.py</code> <pre><code>class PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding.\"\"\"\n\n    def __init__(self, d_model: int, max_len: int = 32):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]\n</code></pre>"},{"location":"api/models/transformer/#pitch_sequencing.models.transformer.TransformerModel","title":"<code>TransformerModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Transformer wrapper implementing BaseModel interface.</p> Source code in <code>src/pitch_sequencing/models/transformer.py</code> <pre><code>class TransformerModel(BaseModel):\n    \"\"\"Transformer wrapper implementing BaseModel interface.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.d_model = config.get(\"d_model\", 64)\n        self.nhead = config.get(\"nhead\", 4)\n        self.num_layers = config.get(\"num_layers\", 2)\n        self.dim_feedforward = config.get(\"dim_feedforward\", 128)\n        self.dropout = config.get(\"dropout\", 0.2)\n        self.epochs = config.get(\"epochs\", 30)\n        self.lr = config.get(\"learning_rate\", 0.0005)\n        self.batch_size = config.get(\"batch_size\", 256)\n        self._model = None\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._history = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"Transformer\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        input_features = X_train.shape[2]\n        num_classes = len(np.unique(y_train))\n\n        self._model = PitchTransformer(\n            input_features=input_features,\n            num_classes=num_classes,\n            d_model=self.d_model,\n            nhead=self.nhead,\n            num_layers=self.num_layers,\n            dim_feedforward=self.dim_feedforward,\n            dropout=self.dropout,\n        )\n\n        train_ds = PitchSequenceDataset(X_train, y_train)\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n\n        if X_val is not None and y_val is not None:\n            val_ds = PitchSequenceDataset(X_val, y_val)\n        else:\n            split = int(len(X_train) * 0.8)\n            val_ds = PitchSequenceDataset(X_train[split:], y_train[split:])\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n\n        self._history = train_torch_model(\n            self._model, train_loader, val_loader,\n            epochs=self.epochs, lr=self.lr, device=self._device,\n        )\n\n    def predict(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        preds, _ = predict_torch_model(self._model, loader, self._device)\n        return preds\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        _, probs = predict_torch_model(self._model, loader, self._device)\n        return probs\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"d_model\": self.d_model,\n            \"nhead\": self.nhead,\n            \"num_layers\": self.num_layers,\n            \"dim_feedforward\": self.dim_feedforward,\n            \"dropout\": self.dropout,\n            \"epochs\": self.epochs,\n            \"learning_rate\": self.lr,\n        }\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or later</li> <li>pip (included with Python)</li> <li>Git (for cloning the repository)</li> </ul>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code>git clone https://github.com/jman4162/Baseball-Pitch-Sequence-Prediction.git\ncd Baseball-Pitch-Sequence-Prediction\n</code></pre>"},{"location":"getting-started/installation/#development-install-recommended","title":"Development Install (Recommended)","text":"<p>Includes all optional dependencies and development tools:</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -e \".[all,dev]\"\n</code></pre> <p>Or using Make:</p> <pre><code>make install\n</code></pre>"},{"location":"getting-started/installation/#minimal-install","title":"Minimal Install","text":"<p>Core dependencies only (no AutoGluon or hmmlearn):</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting-started/installation/#optional-extras","title":"Optional Extras","text":"Extra Contents <code>all</code> AutoGluon + hmmlearn <code>autogluon</code> AutoGluon TabularPredictor <code>hmm</code> hmmlearn for HMM model <code>docs</code> MkDocs + Material theme + mkdocstrings <code>dev</code> pytest + build + docs <p>Install specific extras:</p> <pre><code>pip install -e \".[hmm]\"         # Just HMM support\npip install -e \".[autogluon]\"   # Just AutoGluon\npip install -e \".[docs]\"        # Documentation tools\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Check the package is installed\npython -c \"import pitch_sequencing; print(pitch_sequencing.__version__)\"\n\n# Check CLI commands are available\npitch-generate --help\npitch-train --help\npitch-benchmark --help\npitch-ablation --help\n</code></pre>"},{"location":"getting-started/installation/#generate-training-data","title":"Generate Training Data","text":"<p>After installation, generate the synthetic dataset:</p> <pre><code>pitch-generate --output-dir ./data\n</code></pre> <p>This creates two files in <code>data/</code>:</p> <ul> <li><code>baseball_pitch_data.csv</code> (~384K rows)</li> <li><code>synthetic_pitch_sequences.csv</code> (2,500 sequences)</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide walks you through generating data, training a model, and running the benchmark suite.</p>"},{"location":"getting-started/quickstart/#setup","title":"Setup","text":"<pre><code>python -m venv venv\nsource venv/bin/activate\nmake install  # pip install -e \".[all,dev]\"\n</code></pre>"},{"location":"getting-started/quickstart/#1-generate-synthetic-data","title":"1. Generate Synthetic Data","text":"<pre><code>make data\n# or: pitch-generate --num-games 3000 --at-bats 35 --output-dir ./data\n</code></pre> <p>This produces ~384K pitch rows with realistic pitcher archetypes, sequence strategies, fatigue modeling, and game context.</p>"},{"location":"getting-started/quickstart/#2-train-a-single-model","title":"2. Train a Single Model","text":"<pre><code>make train MODEL=lstm\n# or: pitch-train --model lstm\n</code></pre> <p>Available models: <code>logistic_regression</code>, <code>random_forest</code>, <code>hmm</code>, <code>autogluon</code>, <code>lstm</code>, <code>cnn1d</code>, <code>transformer</code></p>"},{"location":"getting-started/quickstart/#3-run-the-full-benchmark","title":"3. Run the Full Benchmark","text":"<pre><code>make benchmark\n# or: pitch-benchmark\n</code></pre> <p>This runs all 7 models through 5-fold cross-validation and reports accuracy, F1, and other metrics with bootstrap confidence intervals.</p>"},{"location":"getting-started/quickstart/#4-run-ablation-studies","title":"4. Run Ablation Studies","text":"<pre><code>make ablation TYPE=feature\n# or: pitch-ablation --type feature --model lstm\n</code></pre> <p>Ablation types: <code>feature</code>, <code>architecture</code>, <code>data</code>, <code>hyperparam</code></p>"},{"location":"getting-started/quickstart/#5-view-results-in-mlflow","title":"5. View Results in MLflow","text":"<pre><code>make mlflow\n# Opens at http://localhost:5000\n</code></pre>"},{"location":"getting-started/quickstart/#python-api","title":"Python API","text":"<p>You can also use the package programmatically:</p> <pre><code>from pitch_sequencing import load_pitch_data, get_model, MODEL_REGISTRY\n\n# Load data\ndf = load_pitch_data(\"data/baseball_pitch_data.csv\")\n\n# List available models\nprint(list(MODEL_REGISTRY.keys()))\n\n# Create and train a model\nmodel = get_model(\"random_forest\", {\"n_estimators\": 200, \"max_depth\": 15})\n# ... prepare X_train, y_train ...\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre> <p>See the User Guide for a complete walkthrough of the data pipeline, training, and evaluation.</p>"},{"location":"models/","title":"Models Overview","text":"<p>All 7 models implement the <code>BaseModel</code> abstract interface and are accessible through the <code>MODEL_REGISTRY</code>.</p>"},{"location":"models/#model-comparison","title":"Model Comparison","text":"Model Type Key Config Strengths Logistic Regression Tabular C=1.0, balanced weights Fast, interpretable baseline Random Forest Tabular 200 trees, max_depth=15 Handles non-linear relationships HMM Sequence 1-8 hidden states Captures latent pitch states AutoGluon Tabular good_quality preset Automated model selection LSTM Sequence 2-layer, hidden=64, window=8 Long-range sequence dependencies 1D-CNN Sequence 3 conv layers, kernel=3 Local pattern detection Transformer Sequence d_model=64, 4 heads, 2 layers Self-attention over sequences"},{"location":"models/#basemodel-interface","title":"BaseModel Interface","text":"<p>All models implement the following abstract interface:</p> <pre><code>from pitch_sequencing.models.base import BaseModel\n\nclass BaseModel(ABC):\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Human-readable model name.\"\"\"\n\n    @property\n    def model_type(self) -&gt; str:\n        \"\"\"'tabular' or 'sequence' \u2014 determines input shape.\"\"\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        \"\"\"Train the model.\"\"\"\n\n    def predict(self, X) -&gt; np.ndarray:\n        \"\"\"Return predicted class labels.\"\"\"\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        \"\"\"Return class probabilities (n_samples x n_classes).\"\"\"\n\n    def get_params(self) -&gt; dict:\n        \"\"\"Return model hyperparameters.\"\"\"\n</code></pre>"},{"location":"models/#input-shapes","title":"Input Shapes","text":"<ul> <li>Tabular models (<code>model_type = \"tabular\"</code>): Expect <code>(n_samples, n_features)</code> NumPy arrays or DataFrames</li> <li>Sequence models (<code>model_type = \"sequence\"</code>): Expect <code>(n_samples, window_size, n_features)</code> 3D arrays</li> </ul>"},{"location":"models/#model-registry","title":"Model Registry","text":"<p>Models are accessed by name through the registry:</p> <pre><code>from pitch_sequencing import get_model, MODEL_REGISTRY\n\n# List all registered models\nprint(list(MODEL_REGISTRY.keys()))\n# ['logistic_regression', 'random_forest', 'hmm', 'autogluon', 'lstm', 'cnn1d', 'transformer']\n\n# Instantiate a model with config\nmodel = get_model(\"lstm\", {\"hidden_size\": 64, \"num_layers\": 2})\n</code></pre>"},{"location":"models/autogluon/","title":"AutoGluon","text":"<p>An AutoML model using AutoGluon's TabularPredictor for automated model selection and ensembling.</p>"},{"location":"models/autogluon/#overview","title":"Overview","text":"<ul> <li>Type: Tabular</li> <li>Library: AutoGluon</li> <li>Registry name: <code>autogluon</code></li> <li>Class: <code>AutoGluonModel</code></li> </ul> <p>Note</p> <p>Requires the <code>autogluon</code> optional extra: <code>pip install -e \".[autogluon]\"</code></p>"},{"location":"models/autogluon/#configuration","title":"Configuration","text":"<pre><code># configs/models/autogluon.yaml\nmodel_type: autogluon\npreset: good_quality\ntime_limit: null\nmodels_dir: autogluon_pitchtype_models\n</code></pre> Parameter Default Description <code>preset</code> <code>good_quality</code> AutoGluon quality preset <code>time_limit</code> <code>null</code> Max training time in seconds (null = no limit) <code>models_dir</code> <code>autogluon_pitchtype_models</code> Directory for AutoGluon model artifacts"},{"location":"models/autogluon/#usage","title":"Usage","text":"<pre><code>from pitch_sequencing import get_model\n\nmodel = get_model(\"autogluon\", {\"preset\": \"good_quality\"})\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"models/autogluon/#api-reference","title":"API Reference","text":""},{"location":"models/autogluon/#pitch_sequencing.models.autogluon_model.AutoGluonModel","title":"<code>pitch_sequencing.models.autogluon_model.AutoGluonModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>AutoGluon TabularPredictor for tabular pitch data.</p> Source code in <code>src/pitch_sequencing/models/autogluon_model.py</code> <pre><code>class AutoGluonModel(BaseModel):\n    \"\"\"AutoGluon TabularPredictor for tabular pitch data.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.preset = config.get(\"preset\", \"good_quality\")\n        self.time_limit = config.get(\"time_limit\", None)\n        self.models_dir = config.get(\"models_dir\", \"autogluon_pitchtype_models\")\n        self._predictor = None\n        self._label = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"AutoGluon\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"tabular\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        from autogluon.tabular import TabularDataset, TabularPredictor\n\n        self._label = y_train.name if hasattr(y_train, \"name\") else \"target\"\n        train_df = pd.DataFrame(X_train).copy()\n        train_df[self._label] = y_train.values if hasattr(y_train, \"values\") else y_train\n        train_data = TabularDataset(train_df)\n\n        fit_kwargs = {\"presets\": self.preset}\n        if self.time_limit is not None:\n            fit_kwargs[\"time_limit\"] = self.time_limit\n\n        self._predictor = TabularPredictor(\n            label=self._label, path=self.models_dir\n        ).fit(train_data, **fit_kwargs)\n\n    def predict(self, X) -&gt; np.ndarray:\n        from autogluon.tabular import TabularDataset\n\n        test_df = pd.DataFrame(X)\n        return self._predictor.predict(TabularDataset(test_df)).values\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        from autogluon.tabular import TabularDataset\n\n        test_df = pd.DataFrame(X)\n        proba = self._predictor.predict_proba(TabularDataset(test_df))\n        return proba.values\n\n    def get_params(self) -&gt; dict:\n        return {\"preset\": self.preset, \"time_limit\": self.time_limit}\n</code></pre>"},{"location":"models/cnn1d/","title":"1D-CNN","text":"<p>A 3-layer 1D convolutional neural network for detecting local pitch sequence patterns.</p>"},{"location":"models/cnn1d/#overview","title":"Overview","text":"<ul> <li>Type: Sequence</li> <li>Library: PyTorch</li> <li>Registry name: <code>cnn1d</code></li> <li>Class: <code>CNN1DModel</code></li> <li>Network: <code>PitchCNN1D</code></li> </ul>"},{"location":"models/cnn1d/#architecture","title":"Architecture","text":"<pre><code>Input (batch, window_size, n_features)\n    \u2192 Transpose to (batch, n_features, window_size)\n    \u2192 Conv1d(n_features, 64, kernel=3) + ReLU + BatchNorm\n    \u2192 Conv1d(64, 128, kernel=3) + ReLU + BatchNorm\n    \u2192 Conv1d(128, 64, kernel=3) + ReLU + BatchNorm\n    \u2192 Adaptive Max Pooling \u2192 Dropout\n    \u2192 Fully connected \u2192 num_classes\n</code></pre>"},{"location":"models/cnn1d/#configuration","title":"Configuration","text":"<pre><code># configs/models/cnn1d.yaml\nmodel_type: cnn1d\nfilters: [64, 128, 64]\nkernel_size: 3\ndropout: 0.3\nepochs: 20\nlearning_rate: 0.001\nbatch_size: 256\n</code></pre> Parameter Default Description <code>filters</code> <code>[64, 128, 64]</code> Number of filters per conv layer <code>kernel_size</code> 3 Convolution kernel size <code>dropout</code> 0.3 Dropout rate <code>epochs</code> 20 Maximum training epochs <code>learning_rate</code> 0.001 Adam optimizer learning rate <code>batch_size</code> 256 Training batch size <code>patience</code> 5 Early stopping patience"},{"location":"models/cnn1d/#usage","title":"Usage","text":"<pre><code>from pitch_sequencing import get_model\n\nmodel = get_model(\"cnn1d\", {\n    \"filters\": [64, 128, 64],\n    \"kernel_size\": 3,\n    \"epochs\": 20\n})\n\nmodel.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"models/cnn1d/#api-reference","title":"API Reference","text":""},{"location":"models/cnn1d/#pitch_sequencing.models.cnn1d.CNN1DModel","title":"<code>pitch_sequencing.models.cnn1d.CNN1DModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>1D-CNN wrapper implementing BaseModel interface.</p> Source code in <code>src/pitch_sequencing/models/cnn1d.py</code> <pre><code>class CNN1DModel(BaseModel):\n    \"\"\"1D-CNN wrapper implementing BaseModel interface.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.filters = config.get(\"filters\", [64, 128, 64])\n        self.kernel_size = config.get(\"kernel_size\", 3)\n        self.dropout = config.get(\"dropout\", 0.3)\n        self.epochs = config.get(\"epochs\", 30)\n        self.lr = config.get(\"learning_rate\", 0.001)\n        self.batch_size = config.get(\"batch_size\", 256)\n        self._model = None\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._history = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"1D-CNN\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        input_features = X_train.shape[2]\n        num_classes = len(np.unique(y_train))\n\n        self._model = PitchCNN1D(\n            input_features=input_features,\n            num_classes=num_classes,\n            filters=self.filters,\n            kernel_size=self.kernel_size,\n            dropout=self.dropout,\n        )\n\n        train_ds = PitchSequenceDataset(X_train, y_train)\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n\n        if X_val is not None and y_val is not None:\n            val_ds = PitchSequenceDataset(X_val, y_val)\n        else:\n            split = int(len(X_train) * 0.8)\n            val_ds = PitchSequenceDataset(X_train[split:], y_train[split:])\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n\n        self._history = train_torch_model(\n            self._model, train_loader, val_loader,\n            epochs=self.epochs, lr=self.lr, device=self._device,\n        )\n\n    def predict(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        preds, _ = predict_torch_model(self._model, loader, self._device)\n        return preds\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        _, probs = predict_torch_model(self._model, loader, self._device)\n        return probs\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"filters\": self.filters,\n            \"kernel_size\": self.kernel_size,\n            \"dropout\": self.dropout,\n            \"epochs\": self.epochs,\n            \"learning_rate\": self.lr,\n        }\n</code></pre>"},{"location":"models/hmm/","title":"Hidden Markov Model (HMM)","text":"<p>A sequence model using hmmlearn's CategoricalHMM to capture latent pitch states.</p>"},{"location":"models/hmm/#overview","title":"Overview","text":"<ul> <li>Type: Sequence</li> <li>Library: hmmlearn</li> <li>Registry name: <code>hmm</code></li> <li>Class: <code>HMMModel</code></li> </ul> <p>Note</p> <p>Requires the <code>hmm</code> optional extra: <code>pip install -e \".[hmm]\"</code></p>"},{"location":"models/hmm/#configuration","title":"Configuration","text":"<pre><code># configs/models/hmm.yaml\nmodel_type: hmm\nmin_components: 1\nmax_components: 8\nn_iter: 100\n</code></pre> Parameter Default Description <code>min_components</code> 1 Minimum hidden states to try <code>max_components</code> 8 Maximum hidden states to try <code>n_iter</code> 100 EM iterations per fit"},{"location":"models/hmm/#how-it-works","title":"How It Works","text":"<p>During <code>fit()</code>, the model sweeps the number of hidden states from <code>min_components</code> to <code>max_components</code>, training a CategoricalHMM for each value. The best model is selected by validation accuracy.</p>"},{"location":"models/hmm/#usage","title":"Usage","text":"<pre><code>from pitch_sequencing import get_model\n\nmodel = get_model(\"hmm\", {\"min_components\": 1, \"max_components\": 8})\nmodel.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"models/hmm/#api-reference","title":"API Reference","text":""},{"location":"models/hmm/#pitch_sequencing.models.hmm_model.HMMModel","title":"<code>pitch_sequencing.models.hmm_model.HMMModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Hidden Markov Model for pitch sequence prediction.</p> Source code in <code>src/pitch_sequencing/models/hmm_model.py</code> <pre><code>class HMMModel(BaseModel):\n    \"\"\"Hidden Markov Model for pitch sequence prediction.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.min_components = config.get(\"min_components\", 1)\n        self.max_components = config.get(\"max_components\", 8)\n        self.n_iter = config.get(\"n_iter\", 100)\n        self._model = None\n        self._best_n = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"HMM\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        \"\"\"Train HMM by sweeping n_components and picking best by validation accuracy.\n\n        For HMM, X_train is expected to be a flat 2D array of shape (n_samples, 1)\n        with encoded pitch types (the HMM uses its own flat encoding, not windowed).\n        y_train is the same flat array (self-supervised next-token prediction).\n        \"\"\"\n        from hmmlearn import hmm as hmmlearn_hmm\n\n        best_accuracy = 0\n        best_model = None\n\n        for n_components in range(self.min_components, self.max_components + 1):\n            model = hmmlearn_hmm.CategoricalHMM(\n                n_components=n_components,\n                n_iter=self.n_iter,\n                random_state=42,\n            )\n            model.fit(X_train)\n\n            if X_val is not None:\n                predicted = model.predict(X_val)\n                actual = X_val.flatten()\n                accuracy = np.mean(predicted == actual)\n            else:\n                predicted = model.predict(X_train)\n                actual = X_train.flatten()\n                accuracy = np.mean(predicted == actual)\n\n            if accuracy &gt; best_accuracy:\n                best_accuracy = accuracy\n                best_model = model\n                self._best_n = n_components\n\n        self._model = best_model\n\n    def predict(self, X) -&gt; np.ndarray:\n        return self._model.predict(X)\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        \"\"\"Return emission probabilities for each sample given predicted state.\"\"\"\n        states = self._model.predict(X)\n        emission = self._model.emissionprob_\n        return emission[states]\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"n_components\": self._best_n,\n            \"min_components\": self.min_components,\n            \"max_components\": self.max_components,\n            \"n_iter\": self.n_iter,\n        }\n</code></pre>"},{"location":"models/hmm/#pitch_sequencing.models.hmm_model.HMMModel.fit","title":"<code>fit(X_train, y_train, X_val=None, y_val=None, **kwargs)</code>","text":"<p>Train HMM by sweeping n_components and picking best by validation accuracy.</p> <p>For HMM, X_train is expected to be a flat 2D array of shape (n_samples, 1) with encoded pitch types (the HMM uses its own flat encoding, not windowed). y_train is the same flat array (self-supervised next-token prediction).</p> Source code in <code>src/pitch_sequencing/models/hmm_model.py</code> <pre><code>def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n    \"\"\"Train HMM by sweeping n_components and picking best by validation accuracy.\n\n    For HMM, X_train is expected to be a flat 2D array of shape (n_samples, 1)\n    with encoded pitch types (the HMM uses its own flat encoding, not windowed).\n    y_train is the same flat array (self-supervised next-token prediction).\n    \"\"\"\n    from hmmlearn import hmm as hmmlearn_hmm\n\n    best_accuracy = 0\n    best_model = None\n\n    for n_components in range(self.min_components, self.max_components + 1):\n        model = hmmlearn_hmm.CategoricalHMM(\n            n_components=n_components,\n            n_iter=self.n_iter,\n            random_state=42,\n        )\n        model.fit(X_train)\n\n        if X_val is not None:\n            predicted = model.predict(X_val)\n            actual = X_val.flatten()\n            accuracy = np.mean(predicted == actual)\n        else:\n            predicted = model.predict(X_train)\n            actual = X_train.flatten()\n            accuracy = np.mean(predicted == actual)\n\n        if accuracy &gt; best_accuracy:\n            best_accuracy = accuracy\n            best_model = model\n            self._best_n = n_components\n\n    self._model = best_model\n</code></pre>"},{"location":"models/hmm/#pitch_sequencing.models.hmm_model.HMMModel.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Return emission probabilities for each sample given predicted state.</p> Source code in <code>src/pitch_sequencing/models/hmm_model.py</code> <pre><code>def predict_proba(self, X) -&gt; np.ndarray:\n    \"\"\"Return emission probabilities for each sample given predicted state.\"\"\"\n    states = self._model.predict(X)\n    emission = self._model.emissionprob_\n    return emission[states]\n</code></pre>"},{"location":"models/logistic-regression/","title":"Logistic Regression","text":"<p>A scikit-learn logistic regression classifier serving as a baseline tabular model.</p>"},{"location":"models/logistic-regression/#overview","title":"Overview","text":"<ul> <li>Type: Tabular</li> <li>Library: scikit-learn</li> <li>Registry name: <code>logistic_regression</code></li> <li>Class: <code>LogisticRegressionModel</code></li> </ul>"},{"location":"models/logistic-regression/#configuration","title":"Configuration","text":"<pre><code># configs/models/logistic.yaml\nmodel_type: logistic_regression\nC: 1.0\npenalty: l2\nclass_weight: balanced\nmax_iter: 1000\n</code></pre> Parameter Default Description <code>C</code> 1.0 Inverse regularization strength <code>penalty</code> <code>l2</code> Regularization type <code>class_weight</code> <code>balanced</code> Adjusts weights inversely proportional to class frequencies <code>max_iter</code> 1000 Maximum iterations for solver"},{"location":"models/logistic-regression/#usage","title":"Usage","text":"<pre><code>from pitch_sequencing import get_model\n\nmodel = get_model(\"logistic_regression\", {\"C\": 1.0, \"class_weight\": \"balanced\"})\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"models/logistic-regression/#api-reference","title":"API Reference","text":""},{"location":"models/logistic-regression/#pitch_sequencing.models.baselines.LogisticRegressionModel","title":"<code>pitch_sequencing.models.baselines.LogisticRegressionModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Logistic Regression baseline for tabular pitch data.</p> Source code in <code>src/pitch_sequencing/models/baselines.py</code> <pre><code>class LogisticRegressionModel(BaseModel):\n    \"\"\"Logistic Regression baseline for tabular pitch data.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.C = config.get(\"C\", 1.0)\n        self.penalty = config.get(\"penalty\", \"l2\")\n        self.class_weight = config.get(\"class_weight\", \"balanced\")\n        self.max_iter = config.get(\"max_iter\", 1000)\n        self._model = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"Logistic Regression\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"tabular\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        self._model = LogisticRegression(\n            C=self.C,\n            penalty=self.penalty,\n            class_weight=self.class_weight,\n            max_iter=self.max_iter,\n            solver=\"lbfgs\",\n            multi_class=\"multinomial\",\n        )\n        self._model.fit(X_train, y_train)\n\n    def predict(self, X) -&gt; np.ndarray:\n        return self._model.predict(X)\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        return self._model.predict_proba(X)\n\n    def get_params(self) -&gt; dict:\n        return {\"C\": self.C, \"penalty\": self.penalty, \"class_weight\": self.class_weight}\n</code></pre>"},{"location":"models/lstm/","title":"LSTM","text":"<p>A 2-layer Long Short-Term Memory neural network for sequence-based pitch prediction.</p>"},{"location":"models/lstm/#overview","title":"Overview","text":"<ul> <li>Type: Sequence</li> <li>Library: PyTorch</li> <li>Registry name: <code>lstm</code></li> <li>Class: <code>LSTMModel</code></li> <li>Network: <code>PitchPredictor</code></li> </ul>"},{"location":"models/lstm/#architecture","title":"Architecture","text":"<pre><code>Input (batch, window_size, n_features)\n    \u2192 LSTM (2 layers, hidden_size=64, dropout=0.3)\n    \u2192 Take last hidden state\n    \u2192 Fully connected \u2192 num_classes\n</code></pre>"},{"location":"models/lstm/#configuration","title":"Configuration","text":"<pre><code># configs/models/lstm.yaml\nmodel_type: lstm\nhidden_size: 64\nnum_layers: 2\ndropout: 0.3\nepochs: 20\nlearning_rate: 0.001\nbatch_size: 256\n</code></pre> Parameter Default Description <code>hidden_size</code> 64 LSTM hidden dimension <code>num_layers</code> 2 Number of stacked LSTM layers <code>dropout</code> 0.3 Dropout rate <code>epochs</code> 20 Maximum training epochs <code>learning_rate</code> 0.001 Adam optimizer learning rate <code>batch_size</code> 32 Training batch size <code>patience</code> 5 Early stopping patience"},{"location":"models/lstm/#usage","title":"Usage","text":"<pre><code>from pitch_sequencing import get_model\n\nmodel = get_model(\"lstm\", {\n    \"hidden_size\": 64,\n    \"num_layers\": 2,\n    \"epochs\": 20,\n    \"batch_size\": 256\n})\n\n# X_train shape: (n_samples, window_size, n_features)\nmodel.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"models/lstm/#api-reference","title":"API Reference","text":""},{"location":"models/lstm/#pitch_sequencing.models.lstm.LSTMModel","title":"<code>pitch_sequencing.models.lstm.LSTMModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>LSTM wrapper implementing BaseModel interface.</p> Source code in <code>src/pitch_sequencing/models/lstm.py</code> <pre><code>class LSTMModel(BaseModel):\n    \"\"\"LSTM wrapper implementing BaseModel interface.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.hidden_size = config.get(\"hidden_size\", 64)\n        self.num_layers = config.get(\"num_layers\", 2)\n        self.dropout = config.get(\"dropout\", 0.3)\n        self.epochs = config.get(\"epochs\", 20)\n        self.lr = config.get(\"learning_rate\", 0.001)\n        self.batch_size = config.get(\"batch_size\", 256)\n        self._model = None\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._num_classes = None\n        self._history = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"LSTM\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        input_size = X_train.shape[2]\n        self._num_classes = len(np.unique(y_train))\n\n        self._model = PitchPredictor(\n            input_size=input_size,\n            hidden_size=self.hidden_size,\n            num_layers=self.num_layers,\n            num_classes=self._num_classes,\n            dropout=self.dropout,\n        )\n\n        train_ds = PitchSequenceDataset(X_train, y_train)\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n\n        if X_val is not None and y_val is not None:\n            val_ds = PitchSequenceDataset(X_val, y_val)\n        else:\n            # Use last 20% of training data as validation\n            split = int(len(X_train) * 0.8)\n            val_ds = PitchSequenceDataset(X_train[split:], y_train[split:])\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n\n        self._history = train_torch_model(\n            self._model, train_loader, val_loader,\n            epochs=self.epochs, lr=self.lr, device=self._device,\n        )\n\n    def predict(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        preds, _ = predict_torch_model(self._model, loader, self._device)\n        return preds\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        _, probs = predict_torch_model(self._model, loader, self._device)\n        return probs\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"hidden_size\": self.hidden_size,\n            \"num_layers\": self.num_layers,\n            \"dropout\": self.dropout,\n            \"epochs\": self.epochs,\n            \"learning_rate\": self.lr,\n            \"batch_size\": self.batch_size,\n        }\n</code></pre>"},{"location":"models/random-forest/","title":"Random Forest","text":"<p>A scikit-learn random forest ensemble classifier for tabular pitch data.</p>"},{"location":"models/random-forest/#overview","title":"Overview","text":"<ul> <li>Type: Tabular</li> <li>Library: scikit-learn</li> <li>Registry name: <code>random_forest</code></li> <li>Class: <code>RandomForestModel</code></li> </ul>"},{"location":"models/random-forest/#configuration","title":"Configuration","text":"<pre><code># configs/models/random_forest.yaml\nmodel_type: random_forest\nn_estimators: 200\nmax_depth: 15\nrandom_state: 42\n</code></pre> Parameter Default Description <code>n_estimators</code> 200 Number of trees in the forest <code>max_depth</code> 15 Maximum tree depth <code>random_state</code> 42 Random seed for reproducibility"},{"location":"models/random-forest/#usage","title":"Usage","text":"<pre><code>from pitch_sequencing import get_model\n\nmodel = get_model(\"random_forest\", {\"n_estimators\": 200, \"max_depth\": 15})\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"models/random-forest/#api-reference","title":"API Reference","text":""},{"location":"models/random-forest/#pitch_sequencing.models.baselines.RandomForestModel","title":"<code>pitch_sequencing.models.baselines.RandomForestModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Random Forest baseline for tabular pitch data.</p> Source code in <code>src/pitch_sequencing/models/baselines.py</code> <pre><code>class RandomForestModel(BaseModel):\n    \"\"\"Random Forest baseline for tabular pitch data.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.n_estimators = config.get(\"n_estimators\", 200)\n        self.max_depth = config.get(\"max_depth\", 15)\n        self.min_samples_split = config.get(\"min_samples_split\", 5)\n        self.class_weight = config.get(\"class_weight\", \"balanced\")\n        self._model = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"Random Forest\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"tabular\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        self._model = RandomForestClassifier(\n            n_estimators=self.n_estimators,\n            max_depth=self.max_depth,\n            min_samples_split=self.min_samples_split,\n            class_weight=self.class_weight,\n            random_state=42,\n            n_jobs=-1,\n        )\n        self._model.fit(X_train, y_train)\n\n    def predict(self, X) -&gt; np.ndarray:\n        return self._model.predict(X)\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        return self._model.predict_proba(X)\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"n_estimators\": self.n_estimators,\n            \"max_depth\": self.max_depth,\n            \"min_samples_split\": self.min_samples_split,\n        }\n</code></pre>"},{"location":"models/transformer/","title":"Transformer","text":"<p>A Transformer encoder network using self-attention for pitch sequence prediction.</p>"},{"location":"models/transformer/#overview","title":"Overview","text":"<ul> <li>Type: Sequence</li> <li>Library: PyTorch</li> <li>Registry name: <code>transformer</code></li> <li>Class: <code>TransformerModel</code></li> <li>Network: <code>PitchTransformer</code></li> </ul>"},{"location":"models/transformer/#architecture","title":"Architecture","text":"<pre><code>Input (batch, window_size, n_features)\n    \u2192 Linear embedding \u2192 d_model dimensions\n    \u2192 Sinusoidal positional encoding\n    \u2192 TransformerEncoder (2 layers, 4 attention heads)\n    \u2192 Mean pooling over sequence\n    \u2192 Fully connected \u2192 num_classes\n</code></pre>"},{"location":"models/transformer/#configuration","title":"Configuration","text":"<pre><code># configs/models/transformer.yaml\nmodel_type: transformer\nd_model: 64\nnhead: 4\nnum_layers: 2\ndropout: 0.3\nepochs: 20\nlearning_rate: 0.001\nbatch_size: 256\n</code></pre> Parameter Default Description <code>d_model</code> 64 Embedding dimension <code>nhead</code> 4 Number of attention heads <code>num_layers</code> 2 Number of Transformer encoder layers <code>dropout</code> 0.3 Dropout rate <code>epochs</code> 20 Maximum training epochs <code>learning_rate</code> 0.001 Adam optimizer learning rate <code>batch_size</code> 256 Training batch size <code>patience</code> 5 Early stopping patience"},{"location":"models/transformer/#usage","title":"Usage","text":"<pre><code>from pitch_sequencing import get_model\n\nmodel = get_model(\"transformer\", {\n    \"d_model\": 64,\n    \"nhead\": 4,\n    \"num_layers\": 2,\n    \"epochs\": 20\n})\n\nmodel.fit(X_train, y_train, X_val=X_val, y_val=y_val)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre>"},{"location":"models/transformer/#api-reference","title":"API Reference","text":""},{"location":"models/transformer/#pitch_sequencing.models.transformer.TransformerModel","title":"<code>pitch_sequencing.models.transformer.TransformerModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Transformer wrapper implementing BaseModel interface.</p> Source code in <code>src/pitch_sequencing/models/transformer.py</code> <pre><code>class TransformerModel(BaseModel):\n    \"\"\"Transformer wrapper implementing BaseModel interface.\"\"\"\n\n    def __init__(self, config=None):\n        config = config or {}\n        self.d_model = config.get(\"d_model\", 64)\n        self.nhead = config.get(\"nhead\", 4)\n        self.num_layers = config.get(\"num_layers\", 2)\n        self.dim_feedforward = config.get(\"dim_feedforward\", 128)\n        self.dropout = config.get(\"dropout\", 0.2)\n        self.epochs = config.get(\"epochs\", 30)\n        self.lr = config.get(\"learning_rate\", 0.0005)\n        self.batch_size = config.get(\"batch_size\", 256)\n        self._model = None\n        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self._history = None\n\n    @property\n    def name(self) -&gt; str:\n        return \"Transformer\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        input_features = X_train.shape[2]\n        num_classes = len(np.unique(y_train))\n\n        self._model = PitchTransformer(\n            input_features=input_features,\n            num_classes=num_classes,\n            d_model=self.d_model,\n            nhead=self.nhead,\n            num_layers=self.num_layers,\n            dim_feedforward=self.dim_feedforward,\n            dropout=self.dropout,\n        )\n\n        train_ds = PitchSequenceDataset(X_train, y_train)\n        train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=True)\n\n        if X_val is not None and y_val is not None:\n            val_ds = PitchSequenceDataset(X_val, y_val)\n        else:\n            split = int(len(X_train) * 0.8)\n            val_ds = PitchSequenceDataset(X_train[split:], y_train[split:])\n        val_loader = DataLoader(val_ds, batch_size=self.batch_size, shuffle=False)\n\n        self._history = train_torch_model(\n            self._model, train_loader, val_loader,\n            epochs=self.epochs, lr=self.lr, device=self._device,\n        )\n\n    def predict(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        preds, _ = predict_torch_model(self._model, loader, self._device)\n        return preds\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        ds = PitchSequenceDataset(X, np.zeros(len(X), dtype=np.int64))\n        loader = DataLoader(ds, batch_size=self.batch_size, shuffle=False)\n        _, probs = predict_torch_model(self._model, loader, self._device)\n        return probs\n\n    def get_params(self) -&gt; dict:\n        return {\n            \"d_model\": self.d_model,\n            \"nhead\": self.nhead,\n            \"num_layers\": self.num_layers,\n            \"dim_feedforward\": self.dim_feedforward,\n            \"dropout\": self.dropout,\n            \"epochs\": self.epochs,\n            \"learning_rate\": self.lr,\n        }\n</code></pre>"},{"location":"user-guide/ablation/","title":"Ablation Studies","text":"<p>Ablation studies measure the contribution of individual components by systematically removing or varying them. The <code>AblationRunner</code> supports four types of ablation.</p>"},{"location":"user-guide/ablation/#cli-usage","title":"CLI Usage","text":"<pre><code># Feature ablation\npitch-ablation --type feature --model lstm\n\n# Architecture ablation\npitch-ablation --type architecture --model lstm\n\n# Data scaling ablation\npitch-ablation --type data --model lstm\n\n# Hyperparameter ablation\npitch-ablation --type hyperparam --model lstm\n</code></pre>"},{"location":"user-guide/ablation/#ablation-types","title":"Ablation Types","text":""},{"location":"user-guide/ablation/#feature-ablation","title":"Feature Ablation","text":"<p>Measures the impact of each input feature by training the model with one feature removed at a time.</p> <pre><code>pitch-ablation --type feature --model lstm\n</code></pre> <p>Output: Accuracy drop when each feature is removed, showing which features contribute most.</p>"},{"location":"user-guide/ablation/#architecture-ablation","title":"Architecture Ablation","text":"<p>Varies architectural parameters (e.g., number of layers, hidden size) to understand their impact.</p> <pre><code>pitch-ablation --type architecture --model lstm\n</code></pre>"},{"location":"user-guide/ablation/#data-scaling-ablation","title":"Data Scaling Ablation","text":"<p>Trains the model on increasing fractions of the dataset to measure how performance scales with data size.</p> <pre><code>pitch-ablation --type data --model lstm\n</code></pre>"},{"location":"user-guide/ablation/#hyperparameter-ablation","title":"Hyperparameter Ablation","text":"<p>Sweeps key hyperparameters to understand sensitivity.</p> <pre><code>pitch-ablation --type hyperparam --model lstm\n</code></pre>"},{"location":"user-guide/ablation/#configuration","title":"Configuration","text":"<pre><code># configs/ablation.yaml\nfeature_ablation:\n  model: lstm\n  features_to_drop:\n    - Balls\n    - Strikes\n    - PitcherType_enc\n    - PreviousPitchType_enc\n\ndata_ablation:\n  model: lstm\n  fractions: [0.1, 0.25, 0.5, 0.75, 1.0]\n\narchitecture_ablation:\n  model: lstm\n  variants:\n    - hidden_size: 32\n    - hidden_size: 64\n    - hidden_size: 128\n\nhyperparam_ablation:\n  model: lstm\n  params:\n    learning_rate: [0.0001, 0.001, 0.01]\n    dropout: [0.1, 0.3, 0.5]\n</code></pre>"},{"location":"user-guide/ablation/#python-api","title":"Python API","text":"<pre><code>from pitch_sequencing.config import DataConfig, AblationConfig\nfrom pitch_sequencing.evaluation.ablation import AblationRunner\n\ndata_cfg = DataConfig.from_yaml(\"configs/data.yaml\")\nablation_cfg = AblationConfig(...)  # Load from YAML or construct\n\nrunner = AblationRunner(ablation_cfg, data_cfg, models_config_dir=\"configs/models\")\n# Run specific ablation type\n</code></pre>"},{"location":"user-guide/ablation/#mlflow-logging","title":"MLflow Logging","text":"<p>All ablation runs are logged to MLflow with parameters identifying the ablation type and variant. Use the MLflow UI to compare results across ablation conditions.</p>"},{"location":"user-guide/benchmarking/","title":"Benchmarking","text":"<p>The benchmark suite runs all models through k-fold cross-validation and computes metrics with bootstrap confidence intervals and statistical tests.</p>"},{"location":"user-guide/benchmarking/#cli-usage","title":"CLI Usage","text":"<pre><code># Run with default config\npitch-benchmark\n\n# Run with custom config\npitch-benchmark --config configs/benchmark.yaml\n</code></pre>"},{"location":"user-guide/benchmarking/#benchmark-configuration","title":"Benchmark Configuration","text":"<pre><code># configs/benchmark.yaml\nexperiment_name: pitch_benchmark\nmodels:\n  - logistic_regression\n  - random_forest\n  - hmm\n  - autogluon\n  - lstm\n  - cnn1d\n  - transformer\nn_folds: 5\nmetrics:\n  - accuracy\n  - balanced_accuracy\n  - macro_f1\n  - log_loss\n</code></pre>"},{"location":"user-guide/benchmarking/#python-api","title":"Python API","text":"<pre><code>from pitch_sequencing.config import DataConfig, BenchmarkConfig\nfrom pitch_sequencing.evaluation.benchmark import BenchmarkRunner\n\ndata_cfg = DataConfig.from_yaml(\"configs/data.yaml\")\nbench_cfg = BenchmarkConfig(\n    experiment_name=\"my_benchmark\",\n    models=[\"lstm\", \"random_forest\", \"transformer\"],\n    n_folds=5,\n    metrics=[\"accuracy\", \"macro_f1\"]\n)\n\nrunner = BenchmarkRunner(bench_cfg, data_cfg, models_config_dir=\"configs/models\")\nresults_df = runner.run()\nprint(results_df)\n</code></pre>"},{"location":"user-guide/benchmarking/#output","title":"Output","text":"<p>The benchmark produces:</p> <ul> <li>Per-fold metrics for each model</li> <li>Bootstrap confidence intervals (95% CI by default, 1000 bootstrap samples)</li> <li>Paired t-tests between model pairs with p-values</li> <li>Cohen's d effect sizes for pairwise comparisons</li> <li>MLflow experiment logs with parameters, metrics, and artifacts</li> </ul>"},{"location":"user-guide/benchmarking/#metrics","title":"Metrics","text":"Metric Description <code>accuracy</code> Overall accuracy <code>balanced_accuracy</code> Average per-class recall <code>macro_precision</code> Macro-averaged precision <code>macro_recall</code> Macro-averaged recall <code>macro_f1</code> Macro-averaged F1 score <code>log_loss</code> Logarithmic loss (requires <code>predict_proba</code>) <p>Per-class precision, recall, and F1 are also computed for each pitch type (Fastball, Slider, Curveball, Changeup).</p>"},{"location":"user-guide/benchmarking/#statistical-comparisons","title":"Statistical Comparisons","text":"<p>After k-fold CV, models are compared pairwise:</p> <ul> <li>Paired t-test: Tests whether the difference in fold scores is statistically significant</li> <li>Cohen's d: Measures the effect size (small: 0.2, medium: 0.5, large: 0.8)</li> </ul>"},{"location":"user-guide/benchmarking/#mlflow-integration","title":"MLflow Integration","text":"<p>All benchmark runs are logged to MLflow under the experiment name. See MLflow Tracking for details on viewing and comparing runs.</p>"},{"location":"user-guide/data-pipeline/","title":"Data Pipeline","text":"<p>The data pipeline generates synthetic baseball pitch data, loads it, preprocesses features, and creates sequences for model training.</p>"},{"location":"user-guide/data-pipeline/#data-generation","title":"Data Generation","text":"<p>The simulator (<code>pitch_sequencing.data.simulator</code>) generates realistic pitch-by-pitch data.</p>"},{"location":"user-guide/data-pipeline/#pitcher-archetypes","title":"Pitcher Archetypes","text":"<p>Each simulated pitcher is assigned one of four archetypes:</p> Archetype Fastball % Slider % Curveball % Changeup % Fatigue Threshold Power 55% 20% 10% 15% 95 pitches Finesse 25% 15% 30% 30% 80 pitches Slider Specialist 20% 40% 20% 20% 85 pitches Balanced 30% 25% 25% 20% 90 pitches <p>Archetype blending uses 60% archetype bias and 40% count-based probabilities.</p>"},{"location":"user-guide/data-pipeline/#sequence-strategies","title":"Sequence Strategies","text":"<p>Eight pitch patterns create learnable sequential dependencies by boosting follow-up pitch probability by 15-25%:</p> <ul> <li>Fastball \u2192 Fastball \u2192 Changeup</li> <li>Slider \u2192 Slider \u2192 Fastball</li> <li>Curveball \u2192 Fastball (and more)</li> </ul>"},{"location":"user-guide/data-pipeline/#count-dependent-outcomes","title":"Count-Dependent Outcomes","text":"<p>Hit rates vary from 5-6% in pitcher's counts (0-2, 1-2) to 19-23% in hitter's counts (3-0, 3-1).</p>"},{"location":"user-guide/data-pipeline/#fatigue-modeling","title":"Fatigue Modeling","text":"<p>After an archetype-specific threshold (80-95 pitches), pitchers shift toward fastballs and more balls.</p>"},{"location":"user-guide/data-pipeline/#game-situation","title":"Game Situation","text":"<p>Runners on base and score differential affect pitch selection probabilities.</p>"},{"location":"user-guide/data-pipeline/#cli-usage","title":"CLI Usage","text":"<pre><code>pitch-generate --num-games 3000 --at-bats 35 --seed 42 --output-dir ./data\n</code></pre>"},{"location":"user-guide/data-pipeline/#python-usage","title":"Python Usage","text":"<pre><code>from pitch_sequencing.data.simulator import generate_dataset, generate_hmm_sequences\n\n# Main dataset (~384K rows)\ndf = generate_dataset(num_games=3000, at_bats_per_game=35, seed=42)\n\n# HMM sequences (2500 x 100)\nhmm_df = generate_hmm_sequences(num_sequences=2500, sequence_length=100, seed=42)\n</code></pre>"},{"location":"user-guide/data-pipeline/#data-loading","title":"Data Loading","text":"<pre><code>from pitch_sequencing.data.loader import load_pitch_data, create_sequences\n\ndf = load_pitch_data(\"data/baseball_pitch_data.csv\", filter_none_prev=True)\n</code></pre>"},{"location":"user-guide/data-pipeline/#dataset-columns","title":"Dataset Columns","text":"Column Type Description Balls int Current ball count (0-3) Strikes int Current strike count (0-2) PitchType str Fastball, Slider, Curveball, Changeup Outcome str ball, strike, hit PitcherType str power, finesse, slider_specialist, balanced PitchNumber int Cumulative per-game pitch count AtBatNumber int At-bat number within game (1-35) RunnersOn int Number of runners on base (0-3) ScoreDiff int Score differential PreviousPitchType str Previous pitch thrown <p>Note</p> <p><code>PitchNumber</code> is the same value for all pitches within an at-bat. It is a cumulative per-game pitch count, not sequential per-pitch.</p>"},{"location":"user-guide/data-pipeline/#preprocessing","title":"Preprocessing","text":""},{"location":"user-guide/data-pipeline/#encoding-categoricals","title":"Encoding Categoricals","text":"<pre><code>from pitch_sequencing.data.preprocessing import encode_categoricals\n\ndf, encoders = encode_categoricals(df, [\"PitchType\", \"Outcome\", \"PitcherType\", \"PreviousPitchType\"])\n# Creates PitchType_enc, Outcome_enc, etc.\n</code></pre>"},{"location":"user-guide/data-pipeline/#normalizing-numericals","title":"Normalizing Numericals","text":"<pre><code>from pitch_sequencing.data.preprocessing import normalize_numericals\n\ndf, stats = normalize_numericals(df, [\"PitchNumber\", \"AtBatNumber\", \"RunnersOn\", \"ScoreDiff\"])\n# Saves PitchNumber_raw, AtBatNumber_raw for boundary detection\n</code></pre>"},{"location":"user-guide/data-pipeline/#creating-sequences","title":"Creating Sequences","text":"<p>For sequence models (LSTM, CNN1D, Transformer):</p> <pre><code>from pitch_sequencing.data.loader import create_sequences\n\nX_seq, y_seq, game_starts = create_sequences(\n    df, window_size=8,\n    feature_cols=[\"Balls\", \"Strikes\", \"PitchType_enc\", ...],\n    target_col=\"PitchType_enc\"\n)\n# X_seq shape: (n_samples, 8, n_features)\n# y_seq shape: (n_samples,)\n</code></pre> <p>Game boundaries are detected using AtBatNumber resets (drops from ~35 back to 1).</p>"},{"location":"user-guide/data-pipeline/#creating-traintest-splits","title":"Creating Train/Test Splits","text":"<pre><code>from pitch_sequencing.data.preprocessing import create_splits\n\nfolds = create_splits(X, y, n_folds=5, stratify=True, random_state=42)\nfor train_idx, test_idx in folds:\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n</code></pre>"},{"location":"user-guide/mlflow/","title":"MLflow Tracking","text":"<p>All experiments are tracked with MLflow for reproducibility and comparison.</p>"},{"location":"user-guide/mlflow/#starting-the-ui","title":"Starting the UI","text":"<pre><code>make mlflow\n# or: mlflow ui --backend-store-uri experiments\n</code></pre> <p>This opens the MLflow UI at http://localhost:5000.</p>"},{"location":"user-guide/mlflow/#tracking-uri","title":"Tracking URI","text":"<p>Experiments are stored locally in the <code>experiments/</code> directory (gitignored):</p> <pre><code>file://./experiments/\n</code></pre>"},{"location":"user-guide/mlflow/#what-gets-logged","title":"What Gets Logged","text":""},{"location":"user-guide/mlflow/#benchmark-runs","title":"Benchmark Runs","text":"<p>Each model's k-fold CV run logs:</p> <ul> <li>Parameters: model name, hyperparameters, n_folds, data config</li> <li>Metrics: accuracy, balanced_accuracy, macro_f1, log_loss (per fold and averaged)</li> <li>Artifacts: results DataFrame, confusion matrices</li> </ul>"},{"location":"user-guide/mlflow/#ablation-runs","title":"Ablation Runs","text":"<p>Each ablation variant logs:</p> <ul> <li>Parameters: ablation type, model, variant description</li> <li>Metrics: performance under each ablation condition</li> <li>Tags: ablation type for easy filtering</li> </ul>"},{"location":"user-guide/mlflow/#experiment-names","title":"Experiment Names","text":"Run Type Default Experiment Name Benchmark <code>pitch_benchmark</code> Ablation <code>pitch_ablation</code> Single training <code>pitch_train</code>"},{"location":"user-guide/mlflow/#comparing-runs","title":"Comparing Runs","text":"<p>In the MLflow UI:</p> <ol> <li>Select an experiment from the sidebar</li> <li>Check the runs you want to compare</li> <li>Click Compare to see side-by-side metrics and parameters</li> <li>Use the Chart view to visualize metric distributions</li> </ol>"},{"location":"user-guide/mlflow/#programmatic-access","title":"Programmatic Access","text":"<pre><code>import mlflow\n\nmlflow.set_tracking_uri(\"file://./experiments\")\n\n# List experiments\nfor exp in mlflow.search_experiments():\n    print(exp.name, exp.experiment_id)\n\n# Query runs\nruns = mlflow.search_runs(experiment_names=[\"pitch_benchmark\"])\nprint(runs[[\"params.model\", \"metrics.accuracy\", \"metrics.macro_f1\"]])\n</code></pre>"},{"location":"user-guide/mlflow/#cleaning-up","title":"Cleaning Up","text":"<p>Experiment artifacts are stored in <code>experiments/</code> and are gitignored. To clean up:</p> <pre><code>make clean  # removes experiments/ and other build artifacts\n</code></pre>"},{"location":"user-guide/training/","title":"Training Models","text":""},{"location":"user-guide/training/#model-types","title":"Model Types","text":"<p>Models fall into two categories based on their expected input format:</p> Type Input Shape Models Tabular <code>(n_samples, n_features)</code> Logistic Regression, Random Forest, AutoGluon Sequence <code>(n_samples, window_size, n_features)</code> LSTM, CNN1D, Transformer, HMM"},{"location":"user-guide/training/#cli-training","title":"CLI Training","text":"<pre><code># Train with default config\npitch-train --model lstm\n\n# Train with custom config\npitch-train --model lstm --config configs/models/lstm.yaml\n</code></pre> <p>Available model names: <code>logistic_regression</code>, <code>random_forest</code>, <code>hmm</code>, <code>autogluon</code>, <code>lstm</code>, <code>cnn1d</code>, <code>transformer</code></p>"},{"location":"user-guide/training/#python-api-training","title":"Python API Training","text":""},{"location":"user-guide/training/#tabular-models","title":"Tabular Models","text":"<pre><code>from pitch_sequencing import get_model, load_pitch_data\nfrom pitch_sequencing.data.preprocessing import encode_categoricals, normalize_numericals, create_splits\n\n# Load and preprocess\ndf = load_pitch_data(\"data/baseball_pitch_data.csv\")\ndf, encoders = encode_categoricals(df, [\"PitchType\", \"Outcome\", \"PitcherType\", \"PreviousPitchType\"])\ndf, stats = normalize_numericals(df, [\"PitchNumber\", \"AtBatNumber\", \"RunnersOn\", \"ScoreDiff\"])\n\n# Prepare features\nfeature_cols = [\"Balls\", \"Strikes\", \"PitcherType_enc\", \"PreviousPitchType_enc\",\n                \"PitchNumber\", \"AtBatNumber\", \"RunnersOn\", \"ScoreDiff\"]\nX = df[feature_cols].values\ny = df[\"PitchType_enc\"].values\n\n# Split\nfolds = create_splits(X, y, n_folds=5)\ntrain_idx, test_idx = folds[0]\n\n# Train\nmodel = get_model(\"random_forest\", {\"n_estimators\": 200, \"max_depth\": 15})\nmodel.fit(X[train_idx], y[train_idx])\n\n# Predict\npredictions = model.predict(X[test_idx])\nprobabilities = model.predict_proba(X[test_idx])\n</code></pre>"},{"location":"user-guide/training/#sequence-models","title":"Sequence Models","text":"<pre><code>from pitch_sequencing import get_model\nfrom pitch_sequencing.data.loader import create_sequences\n\n# Create sequences from preprocessed DataFrame\nX_seq, y_seq, _ = create_sequences(df, window_size=8,\n    feature_cols=[\"Balls\", \"Strikes\", \"PitchType_enc\", ...],\n    target_col=\"PitchType_enc\")\n\nfolds = create_splits(X_seq, y_seq, n_folds=5)\ntrain_idx, test_idx = folds[0]\n\n# Train LSTM\nmodel = get_model(\"lstm\", {\"hidden_size\": 64, \"num_layers\": 2, \"epochs\": 20})\nmodel.fit(X_seq[train_idx], y_seq[train_idx],\n          X_val=X_seq[test_idx], y_val=y_seq[test_idx])\n\npredictions = model.predict(X_seq[test_idx])\n</code></pre>"},{"location":"user-guide/training/#model-configuration","title":"Model Configuration","text":"<p>Each model reads hyperparameters from a YAML config file or a dictionary:</p> <pre><code># configs/models/lstm.yaml\nmodel_type: lstm\nhidden_size: 64\nnum_layers: 2\ndropout: 0.3\nepochs: 20\nlearning_rate: 0.001\nbatch_size: 256\n</code></pre> <p>See the Configuration page for all model configs.</p>"},{"location":"user-guide/training/#custom-models","title":"Custom Models","text":"<p>To add a new model, implement the <code>BaseModel</code> abstract class:</p> <pre><code>from pitch_sequencing.models.base import BaseModel\n\nclass MyModel(BaseModel):\n    @property\n    def name(self) -&gt; str:\n        return \"My Custom Model\"\n\n    @property\n    def model_type(self) -&gt; str:\n        return \"tabular\"  # or \"sequence\"\n\n    def fit(self, X_train, y_train, X_val=None, y_val=None, **kwargs):\n        # Training logic\n        pass\n\n    def predict(self, X):\n        # Return class labels\n        pass\n\n    def predict_proba(self, X):\n        # Return (n_samples, n_classes) probability matrix\n        pass\n</code></pre> <p>Then register it in <code>models/__init__.py</code>:</p> <pre><code>MODEL_REGISTRY[\"my_model\"] = MyModel\n</code></pre>"}]}