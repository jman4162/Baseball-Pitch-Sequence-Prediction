{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Baseball Pitch Sequence Predictor\n",
    "\n",
    "Name: John Hodge\n",
    "\n",
    "Date: 02/05/26\n",
    "\n",
    "This notebook uses an LSTM (Long Short-Term Memory) neural network to predict the next pitch type\n",
    "based on a sliding window of recent pitches and game context. LSTMs are well-suited for this task\n",
    "because they can learn sequential patterns — such as pitch setup strategies — that simpler models miss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "We load the enriched pitch data generated by the simulator and create sliding-window sequences.\n",
    "Each sample consists of the last `WINDOW_SIZE` pitches (with context features) and the target\n",
    "is the next pitch type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\ndf = pd.read_csv('data/baseball_pitch_data.csv')\nprint(f'Dataset shape: {df.shape}')\nprint(f'Columns: {list(df.columns)}')\nprint(f'\\nPitch type distribution:')\nprint(df['PitchType'].value_counts(normalize=True).round(3))\ndf.head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "WINDOW_SIZE = 8  # Number of previous pitches to use as input\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "pitch_encoder = LabelEncoder()\n",
    "pitcher_encoder = LabelEncoder()\n",
    "prev_pitch_encoder = LabelEncoder()\n",
    "outcome_encoder = LabelEncoder()\n",
    "\n",
    "df['PitchType_enc'] = pitch_encoder.fit_transform(df['PitchType'])\n",
    "df['PitcherType_enc'] = pitcher_encoder.fit_transform(df['PitcherType'])\n",
    "df['PreviousPitchType_enc'] = prev_pitch_encoder.fit_transform(df['PreviousPitchType'])\n",
    "df['Outcome_enc'] = outcome_encoder.fit_transform(df['Outcome'])\n",
    "\n",
    "NUM_PITCH_TYPES = len(pitch_encoder.classes_)\n",
    "NUM_PITCHER_TYPES = len(pitcher_encoder.classes_)\n",
    "print(f'Pitch types ({NUM_PITCH_TYPES}): {list(pitch_encoder.classes_)}')\n",
    "print(f'Pitcher types ({NUM_PITCHER_TYPES}): {list(pitcher_encoder.classes_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Features to use in the sequence\n# Each timestep has: PitchType_enc, Balls, Strikes, PitcherType_enc, PitchNumber, RunnersOn, ScoreDiff\nFEATURE_COLS = ['PitchType_enc', 'Balls', 'Strikes', 'PitcherType_enc',\n                'PitchNumber', 'RunnersOn', 'ScoreDiff']\nNUM_FEATURES = len(FEATURE_COLS)\n\n# Normalize numerical features\nfor col in ['PitchNumber', 'Balls', 'Strikes', 'ScoreDiff']:\n    df[col] = (df[col] - df[col].mean()) / (df[col].std() + 1e-8)\n\n# Create sliding window sequences\n# We need to respect game boundaries — don't create windows that span across games\n# We can approximate game boundaries by detecting resets in PitchNumber\n# (PitchNumber resets when a new game starts)\n\nfeatures = df[FEATURE_COLS].values\ntargets = df['PitchType_enc'].values\n\n# Detect game boundaries (where raw PitchNumber would drop)\nraw_pitch_num = pd.read_csv('data/baseball_pitch_data.csv')['PitchNumber'].values\ngame_starts = np.where(np.diff(raw_pitch_num, prepend=raw_pitch_num[0]+1) <= 0)[0]\ngame_starts = set(game_starts)\n\nX_sequences = []\ny_targets = []\n\nfor i in range(WINDOW_SIZE, len(features)):\n    # Check that none of the window indices cross a game boundary\n    window_range = range(i - WINDOW_SIZE + 1, i + 1)\n    if any(idx in game_starts for idx in window_range):\n        continue\n    X_sequences.append(features[i - WINDOW_SIZE:i])\n    y_targets.append(targets[i])\n\nX = np.array(X_sequences, dtype=np.float32)\ny = np.array(y_targets, dtype=np.int64)\n\nprint(f'Sequence dataset: X={X.shape}, y={y.shape}')\nprint(f'Each sample: {WINDOW_SIZE} timesteps x {NUM_FEATURES} features')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (80/20, preserving temporal order)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f'Train: {X_train.shape[0]} samples')\n",
    "print(f'Test:  {X_test.shape[0]} samples')\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class PitchSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.FloatTensor(sequences)\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "train_dataset = PitchSequenceDataset(X_train, y_train)\n",
    "test_dataset = PitchSequenceDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the LSTM Model\n",
    "\n",
    "A 2-layer LSTM with dropout. The final hidden state is passed through a fully connected layer\n",
    "to predict the next pitch type (4-class classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PitchPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super(PitchPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # Use the last hidden state\n",
    "        out = self.dropout(h_n[-1])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = PitchPredictor(\n",
    "    input_size=NUM_FEATURES,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_classes=NUM_PITCH_TYPES,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "print(f'\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_test_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "    scheduler.step(avg_test_loss)\n",
    "\n",
    "    if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{EPOCHS}] '\n",
    "              f'Train Loss: {avg_train_loss:.4f} | '\n",
    "              f'Test Loss: {avg_test_loss:.4f} | '\n",
    "              f'Test Acc: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='green')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Test Accuracy Over Epochs')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nFinal test accuracy: {test_accuracies[-1]:.4f}')\n",
    "print(f'Best test accuracy:  {max(test_accuracies):.4f} (epoch {np.argmax(test_accuracies)+1})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(batch_y.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "target_names = list(pitch_encoder.classes_)\n",
    "print('Classification Report:')\n",
    "print(classification_report(all_targets, all_preds, target_names=target_names))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title('LSTM Pitch Prediction - Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Comparison\n",
    "\n",
    "Compare the LSTM against simple baselines:\n",
    "1. **Most frequent**: Always predict the most common pitch type\n",
    "2. **Previous pitch**: Predict whatever pitch was thrown last\n",
    "3. **Random weighted**: Sample from the training distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Most frequent baseline\n",
    "most_common = Counter(y_train).most_common(1)[0][0]\n",
    "most_freq_acc = np.mean(y_test == most_common)\n",
    "\n",
    "# Previous pitch baseline (use the last pitch in each window as prediction)\n",
    "prev_pitch_preds = X_test[:, -1, 0].astype(int)  # PitchType_enc is first feature (but normalized)\n",
    "# We need the raw encoded pitch from the window - use the un-normalized target of the previous step\n",
    "prev_pitch_preds_raw = y[split_idx - 1:len(y) - 1]  # offset by 1 for \"previous\" pitch\n",
    "if len(prev_pitch_preds_raw) > len(y_test):\n",
    "    prev_pitch_preds_raw = prev_pitch_preds_raw[:len(y_test)]\n",
    "prev_pitch_acc = np.mean(y_test[:len(prev_pitch_preds_raw)] == prev_pitch_preds_raw)\n",
    "\n",
    "# Random weighted baseline\n",
    "train_dist = Counter(y_train)\n",
    "total_train = sum(train_dist.values())\n",
    "probs = [train_dist[i] / total_train for i in range(NUM_PITCH_TYPES)]\n",
    "random_preds = np.random.choice(NUM_PITCH_TYPES, size=len(y_test), p=probs)\n",
    "random_acc = np.mean(y_test == random_preds)\n",
    "\n",
    "lstm_acc = test_accuracies[-1]\n",
    "\n",
    "print(f'Baseline Comparison:')\n",
    "print(f'  Most Frequent:  {most_freq_acc:.4f}')\n",
    "print(f'  Previous Pitch: {prev_pitch_acc:.4f}')\n",
    "print(f'  Random Weighted: {random_acc:.4f}')\n",
    "print(f'  LSTM:           {lstm_acc:.4f}')\n",
    "\n",
    "# Bar chart\n",
    "methods = ['Most Frequent', 'Previous Pitch', 'Random Weighted', 'LSTM']\n",
    "accs = [most_freq_acc, prev_pitch_acc, random_acc, lstm_acc]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(methods, accs, color=['gray', 'gray', 'gray', 'steelblue'])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Pitch Prediction: LSTM vs Baselines')\n",
    "for bar, acc in zip(bars, accs):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "plt.ylim(0, max(accs) * 1.15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The LSTM model leverages sequential patterns in the pitch data — particularly the setup-pitch\n",
    "strategies and pitcher archetype tendencies — to predict the next pitch type. By using a sliding\n",
    "window of recent pitches with contextual features (count, pitcher type, fatigue, game situation),\n",
    "the LSTM can capture higher-order dependencies that simpler models like the HMM or count-only\n",
    "tabular models cannot.\n",
    "\n",
    "### Key findings:\n",
    "- The LSTM should outperform the \"most frequent\" baseline, demonstrating that the sequential patterns are learnable\n",
    "- Pitcher archetype is a strong signal — different pitcher types have distinct pitch distributions\n",
    "- Sequence strategies (e.g., Fastball-Fastball-Changeup) create patterns the LSTM can exploit\n",
    "\n",
    "### Potential improvements:\n",
    "- Attention mechanisms to weight which past pitches matter most\n",
    "- Transformer architecture for longer-range dependencies\n",
    "- Embedding layers for categorical features instead of raw encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}